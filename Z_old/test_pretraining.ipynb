{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "approximate-monster",
   "metadata": {},
   "source": [
    "# CUSTOM PRE-TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interpreted-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "designed-warren",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wav2vec2',\n",
       " 'convbert',\n",
       " 'layoutlm',\n",
       " 'distilbert',\n",
       " 'albert',\n",
       " 'bart',\n",
       " 'mbart',\n",
       " 'camembert',\n",
       " 'xlm-roberta',\n",
       " 'longformer',\n",
       " 'roberta',\n",
       " 'squeezebert',\n",
       " 'bert',\n",
       " 'mobilebert',\n",
       " 'flaubert',\n",
       " 'xlm',\n",
       " 'electra',\n",
       " 'reformer',\n",
       " 'funnel',\n",
       " 'mpnet',\n",
       " 'tapas',\n",
       " 'deberta',\n",
       " 'deberta-v2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "MODEL_TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-poker",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pediatric-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataclass of model arguments with default values\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protecting-parker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path='bert-base-uncased', model_type=None, config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, model_revision='main', use_auth_token=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model arguments\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path = 'bert-base-uncased')\n",
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "authorized-inspection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config for pretrained bert-base-uncased\n",
      "Loaded tokenizer for pretrained bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained bert-base-uncased model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer using model arguments\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.config_name:\n",
    "    config = AutoConfig.from_pretrained(model_args.config_name, **config_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)\n",
    "    print(f\"Loaded config for pretrained {model_args.model_name_or_path}\")\n",
    "else:\n",
    "    config = CONFIG_MAPPING[model_args.model_type]()\n",
    "    print(\"WARNING: You are instantiating a new config instance from scratch.\")\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"cache_dir\": model_args.cache_dir,\n",
    "    \"use_fast\": model_args.use_fast_tokenizer,\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "}\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, **tokenizer_kwargs)\n",
    "    print(f\"Loaded tokenizer for pretrained {model_args.model_name_or_path}\")\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "if model_args.model_name_or_path:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    print(f\"Loaded pretrained {model_args.model_name_or_path} model\")\n",
    "else:\n",
    "    print(\"Training new model from scratch\")\n",
    "    model = AutoModelForMaskedLM.from_config(config)\n",
    "\n",
    "# resize token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-arabic",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "prospective-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "political-swing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTrainingArguments(dataset_name=None, dataset_config_name=None, train_file='./train_dummy.csv', validation_file=None, overwrite_cache=False, validation_split_percentage=5, max_seq_length=64, preprocessing_num_workers=None, mlm_probability=0.15, line_by_line=False, pad_to_max_length=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set data training arguments\n",
    "data_args = DataTrainingArguments(\n",
    "    train_file = './train_dummy.csv',\n",
    "    max_seq_length = 64\n",
    ")\n",
    "data_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "auburn-softball",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-79a5364f2bbdc30a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/Paul/.cache/huggingface/datasets/csv/default-79a5364f2bbdc30a/0.0.0/965b6429be0fc05f975b608ce64e1fa941cc8fb4f30629b523d2390f3c0e1a93...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd8592bda09480798bd00313253586a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/Paul/.cache/huggingface/datasets/csv/default-79a5364f2bbdc30a/0.0.0/965b6429be0fc05f975b608ce64e1fa941cc8fb4f30629b523d2390f3c0e1a93. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'text' or the first column. You can easily tweak this\n",
    "# behavior (see below)\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name)\n",
    "    if \"validation\" not in datasets.keys():\n",
    "        datasets[\"validation\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "        )\n",
    "        datasets[\"train\"] = load_dataset(\n",
    "            data_args.dataset_name,\n",
    "            data_args.dataset_config_name,\n",
    "            split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "        )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "    if data_args.validation_file is not None:\n",
    "        data_files[\"validation\"] = data_args.validation_file\n",
    "    extension = data_args.train_file.split(\".\")[-1]\n",
    "    if extension == \"txt\":\n",
    "        extension = \"text\"\n",
    "    datasets = load_dataset(extension, data_files=data_files)\n",
    "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fixed-restaurant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['case_id', 'text', 'prop_hateful', 'label'],\n",
       "    num_rows: 249\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-contractor",
   "metadata": {},
   "source": [
    "## Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "illegal-deficit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir=./Output, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs/Feb25_11-19-21_MacBook-Pro-88.broadband, logging_strategy=LoggingStrategy.STEPS, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=./Output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './Output',\n",
    "    do_train = True,\n",
    "    num_train_epochs = 2\n",
    ")\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "funded-lying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad15ec688df4ad989dc7eec94ed595e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab1f6f44cbf48b8a37df6336d8dd85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "if training_args.do_train:\n",
    "    column_names = datasets[\"train\"].column_names\n",
    "else:\n",
    "    column_names = datasets[\"validation\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "if data_args.max_seq_length is None:\n",
    "    max_seq_length = tokenizer.model_max_length\n",
    "    if max_seq_length > 1024:\n",
    "        logger.warn(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "            \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "        )\n",
    "        max_seq_length = 1024\n",
    "else:\n",
    "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warn(\n",
    "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "if data_args.line_by_line:\n",
    "    # When using line_by_line, we just tokenize each nonempty line.\n",
    "    padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Remove empty lines\n",
    "        examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=padding,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "            # receives the `special_tokens_mask`.\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=[text_column_name],\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "    )\n",
    "else:\n",
    "    # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n",
    "    # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n",
    "    # efficient when it receives the `special_tokens_mask`.\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n",
    "\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "    )\n",
    "\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "    # max_seq_length.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        total_length = (total_length // max_seq_length) * max_seq_length\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "    # might be slower to preprocess.\n",
    "    #\n",
    "    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rubber-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'token_type_ids'],\n",
       "        num_rows: 155\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-cricket",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "complete-apple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 04:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-31-c1dc76ef63c2>, line 71)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-c1dc76ef63c2>\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    return results\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Data collator\n",
    "# This one will take care of randomly masking the tokens.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=data_args.mlm_probability)\n",
    " \n",
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        print(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"] if training_args.do_train else None,\n",
    "    eval_dataset=tokenized_datasets[\"validation\"] if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    if last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    elif model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path):\n",
    "        checkpoint = model_args.model_name_or_path\n",
    "    else:\n",
    "        checkpoint = None\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()  # Saves model and tokenizer\n",
    "\n",
    "    output_train_file = os.path.join(training_args.output_dir, \"train_results.txt\")\n",
    "    if trainer.is_world_process_zero():\n",
    "        with open(output_train_file, \"w\") as writer:\n",
    "            logger.info(\"***** Train results *****\")\n",
    "            for key, value in sorted(train_result.metrics.items()):\n",
    "                logger.info(f\"  {key} = {value}\")\n",
    "                writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "        # Need to save the state, since Trainer.save_model saves only the tokenizer with the model\n",
    "        trainer.state.save_to_json(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if training_args.do_eval:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "    eval_output = trainer.evaluate()\n",
    "\n",
    "    perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "    results[\"perplexity\"] = perplexity\n",
    "\n",
    "    output_eval_file = os.path.join(training_args.output_dir, \"eval_results_mlm.txt\")\n",
    "    if trainer.is_world_process_zero():\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key, value in sorted(results.items()):\n",
    "                logger.info(f\"  {key} = {value}\")\n",
    "                writer.write(f\"{key} = {value}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "        transformers.utils.logging.enable_default_handler()\n",
    "        transformers.utils.logging.enable_explicit_format()\n",
    "    logger.info(\"Training/evaluation parameters %s\", training_args)\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
