{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "disabled-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import Trainer, AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "    \n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "included-commitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../0_models/default-model were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained('../../0_models/default-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "listed-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../../0_models/default-model', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "falling-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fdca24a232e41a05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/Paul/.cache/huggingface/datasets/text/default-fdca24a232e41a05/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acee0040a8542428251e39d4a04d4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /Users/Paul/.cache/huggingface/datasets/text/default-fdca24a232e41a05/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset('text', data_files={'validation': '../../0_data/clean/unlabelled_reddit/politics_test/test_2017_03_5k.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "upper-chester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b0400f734f4e69b6ad3e43932f13a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Remove empty lines\n",
    "    examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
    "    \n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    #remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "transsexual-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "indirect-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "terminal-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "therapeutic-flavor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>masked_token_array_id</th>\n",
       "      <th>masked_token_vocab_id</th>\n",
       "      <th>masked_token_text</th>\n",
       "      <th>top_pred_token_vocab_id</th>\n",
       "      <th>top_pred_token_text</th>\n",
       "      <th>ce_loss</th>\n",
       "      <th>pred_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>2000</td>\n",
       "      <td>to</td>\n",
       "      <td>2521</td>\n",
       "      <td>far</td>\n",
       "      <td>6.496281</td>\n",
       "      <td>[-7.53187, -7.481711, -7.5258093, -7.5237474, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>2562</td>\n",
       "      <td>keep</td>\n",
       "      <td>2185</td>\n",
       "      <td>away</td>\n",
       "      <td>6.685265</td>\n",
       "      <td>[-7.332101, -7.394904, -7.4043255, -7.4488153,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>20687</td>\n",
       "      <td>renew</td>\n",
       "      <td>20410</td>\n",
       "      <td>verify</td>\n",
       "      <td>2.409274</td>\n",
       "      <td>[-4.0348964, -4.034002, -4.3296347, -4.0737686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>[-14.3275175, -14.694819, -14.702918, -14.3263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>39</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>[-9.652537, -9.60094, -9.601458, -9.620717, -9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>99</td>\n",
       "      <td>Neither do I. It's almost like they look at al...</td>\n",
       "      <td>[[CLS], neither, do, i, ., it, ', s, almost, l...</td>\n",
       "      <td>3</td>\n",
       "      <td>1045</td>\n",
       "      <td>i</td>\n",
       "      <td>2205</td>\n",
       "      <td>too</td>\n",
       "      <td>9.134379</td>\n",
       "      <td>[-5.9324603, -5.7664423, -5.801572, -5.7258487...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>99</td>\n",
       "      <td>Neither do I. It's almost like they look at al...</td>\n",
       "      <td>[[CLS], neither, do, i, ., it, ', s, almost, l...</td>\n",
       "      <td>13</td>\n",
       "      <td>2035</td>\n",
       "      <td>all</td>\n",
       "      <td>2035</td>\n",
       "      <td>all</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>[-6.281892, -6.211084, -6.168412, -6.375232, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>99</td>\n",
       "      <td>Neither do I. It's almost like they look at al...</td>\n",
       "      <td>[[CLS], neither, do, i, ., it, ', s, almost, l...</td>\n",
       "      <td>25</td>\n",
       "      <td>19952</td>\n",
       "      <td>guthrie</td>\n",
       "      <td>19952</td>\n",
       "      <td>guthrie</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>[-5.8770943, -6.5148373, -6.2239447, -5.827273...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>99</td>\n",
       "      <td>Neither do I. It's almost like they look at al...</td>\n",
       "      <td>[[CLS], neither, do, i, ., it, ', s, almost, l...</td>\n",
       "      <td>27</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>0.112440</td>\n",
       "      <td>[-6.979398, -7.0463724, -6.777259, -6.7518754,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>99</td>\n",
       "      <td>Neither do I. It's almost like they look at al...</td>\n",
       "      <td>[[CLS], neither, do, i, ., it, ', s, almost, l...</td>\n",
       "      <td>41</td>\n",
       "      <td>4099</td>\n",
       "      <td>enemy</td>\n",
       "      <td>3066</td>\n",
       "      <td>deal</td>\n",
       "      <td>7.026000</td>\n",
       "      <td>[-5.520442, -5.493849, -5.526231, -5.6195107, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case_id                                               text  \\\n",
       "0          0  Before that I was down to just go and vote for...   \n",
       "1          0  Before that I was down to just go and vote for...   \n",
       "2          0  Before that I was down to just go and vote for...   \n",
       "3          0  Before that I was down to just go and vote for...   \n",
       "4          0  Before that I was down to just go and vote for...   \n",
       "..       ...                                                ...   \n",
       "412       99  Neither do I. It's almost like they look at al...   \n",
       "413       99  Neither do I. It's almost like they look at al...   \n",
       "414       99  Neither do I. It's almost like they look at al...   \n",
       "415       99  Neither do I. It's almost like they look at al...   \n",
       "416       99  Neither do I. It's almost like they look at al...   \n",
       "\n",
       "                                        tokenized_text  masked_token_array_id  \\\n",
       "0    [[CLS], before, that, i, was, down, to, just, ...                     14   \n",
       "1    [[CLS], before, that, i, was, down, to, just, ...                     15   \n",
       "2    [[CLS], before, that, i, was, down, to, just, ...                     17   \n",
       "3    [[CLS], before, that, i, was, down, to, just, ...                     26   \n",
       "4    [[CLS], before, that, i, was, down, to, just, ...                     39   \n",
       "..                                                 ...                    ...   \n",
       "412  [[CLS], neither, do, i, ., it, ', s, almost, l...                      3   \n",
       "413  [[CLS], neither, do, i, ., it, ', s, almost, l...                     13   \n",
       "414  [[CLS], neither, do, i, ., it, ', s, almost, l...                     25   \n",
       "415  [[CLS], neither, do, i, ., it, ', s, almost, l...                     27   \n",
       "416  [[CLS], neither, do, i, ., it, ', s, almost, l...                     41   \n",
       "\n",
       "     masked_token_vocab_id masked_token_text  top_pred_token_vocab_id  \\\n",
       "0                     2000                to                     2521   \n",
       "1                     2562              keep                     2185   \n",
       "2                    20687             renew                    20410   \n",
       "3                     1998               and                     1998   \n",
       "4                     1996               the                     1996   \n",
       "..                     ...               ...                      ...   \n",
       "412                   1045                 i                     2205   \n",
       "413                   2035               all                     2035   \n",
       "414                  19952           guthrie                    19952   \n",
       "415                   1998               and                     1998   \n",
       "416                   4099             enemy                     3066   \n",
       "\n",
       "    top_pred_token_text   ce_loss  \\\n",
       "0                   far  6.496281   \n",
       "1                  away  6.685265   \n",
       "2                verify  2.409274   \n",
       "3                   and  0.000039   \n",
       "4                   the  0.123853   \n",
       "..                  ...       ...   \n",
       "412                 too  9.134379   \n",
       "413                 all  0.011680   \n",
       "414             guthrie  0.018343   \n",
       "415                 and  0.112440   \n",
       "416                deal  7.026000   \n",
       "\n",
       "                                           pred_logits  \n",
       "0    [-7.53187, -7.481711, -7.5258093, -7.5237474, ...  \n",
       "1    [-7.332101, -7.394904, -7.4043255, -7.4488153,...  \n",
       "2    [-4.0348964, -4.034002, -4.3296347, -4.0737686...  \n",
       "3    [-14.3275175, -14.694819, -14.702918, -14.3263...  \n",
       "4    [-9.652537, -9.60094, -9.601458, -9.620717, -9...  \n",
       "..                                                 ...  \n",
       "412  [-5.9324603, -5.7664423, -5.801572, -5.7258487...  \n",
       "413  [-6.281892, -6.211084, -6.168412, -6.375232, -...  \n",
       "414  [-5.8770943, -6.5148373, -6.2239447, -5.827273...  \n",
       "415  [-6.979398, -7.0463724, -6.777259, -6.7518754,...  \n",
       "416  [-5.520442, -5.493849, -5.526231, -5.6195107, ...  \n",
       "\n",
       "[417 rows x 10 columns]"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialise dictionary for writing prediction results to\n",
    "out_dict = {\"case_id\": [], \"text\": [], \"tokenized_text\": [],\n",
    "            \"masked_token_array_id\": [], \"masked_token_vocab_id\": [], \"masked_token_text\": [],\n",
    "            \"top_pred_token_vocab_id\": [], \"top_pred_token_text\": [],\n",
    "            \"ce_loss\": [],\n",
    "            \"pred_logits\": []}\n",
    "\n",
    "# set number of shards for splitting dataset into\n",
    "n_shards=100\n",
    "\n",
    "for shard_id in range(2):\n",
    "    \n",
    "    print(shard_id)\n",
    "    \n",
    "    # run prediction on shards of overall test set so as not to exceed RAM\n",
    "    test_shard = tokenized_datasets[\"validation\"].shard(n_shards, shard_id, contiguous=True)\n",
    "    pred_results = trainer.predict(test_shard)\n",
    "\n",
    "    # each row corresponds to a masked token\n",
    "    # first level of iteration is case-by-case\n",
    "    case_id_range = range(shard_id*int((tokenized_datasets[\"validation\"].shape[0]/n_shards)), (shard_id+1)*int((tokenized_datasets[\"validation\"].shape[0]/n_shards)))\n",
    "    \n",
    "    for case_id, result, label_ids in zip(case_id_range, pred_results.predictions, pred_results.label_ids):\n",
    "\n",
    "        # second level of iteration is over masked tokens in a given case    \n",
    "        # not every case necessarily has masked tokens (indicated by label_id not equal to -100)\n",
    "        for masked_token in (label_ids != -100).nonzero()[0]:\n",
    "\n",
    "            # write case_id, text and tokenized text corresponding to a given masked token\n",
    "            out_dict[\"case_id\"].append(case_id)\n",
    "            out_dict[\"text\"].append(tokenized_datasets[\"validation\"][\"text\"][case_id])\n",
    "            out_dict[\"tokenized_text\"].append((tokenizer.convert_ids_to_tokens(tokenized_datasets[\"validation\"][\"input_ids\"][case_id])))\n",
    "\n",
    "            # for each masked token, write out its array id within the text, its vocab id and corresponding text\n",
    "            out_dict[\"masked_token_array_id\"].append(masked_token)\n",
    "            out_dict[\"masked_token_vocab_id\"].append(label_ids[masked_token])\n",
    "            out_dict[\"masked_token_text\"].append(tokenizer.convert_ids_to_tokens([label_ids[masked_token]])[0])\n",
    "\n",
    "            # also write the vocab id and text of the top predicted token\n",
    "            out_dict[\"top_pred_token_vocab_id\"].append(result[masked_token].argmax())\n",
    "            out_dict[\"top_pred_token_text\"].append(tokenizer.convert_ids_to_tokens([result[masked_token].argmax()])[0])\n",
    "\n",
    "            # calculate categorical cross entropy loss as the negative log of the softmax probability of the correct token\n",
    "            ce_loss = -np.log(softmax(result[masked_token])[label_ids[masked_token]])\n",
    "            out_dict[\"ce_loss\"].append(ce_loss)\n",
    "\n",
    "            # save full logits (1xvocab_size) for the masked token for flexibility in further analysis\n",
    "            out_dict[\"pred_logits\"].append(result[masked_token])\n",
    "\n",
    "        \n",
    "# write dataframe from dict    \n",
    "out_df = pd.DataFrame.from_dict(out_dict)\n",
    "out_df\n",
    "\n",
    "# write dataframe to csv\n",
    "#out_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "smooth-narrative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>masked_token_array_id</th>\n",
       "      <th>masked_token_vocab_id</th>\n",
       "      <th>masked_token_text</th>\n",
       "      <th>top_pred_token_vocab_id</th>\n",
       "      <th>top_pred_token_text</th>\n",
       "      <th>ce_loss</th>\n",
       "      <th>pred_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>2000</td>\n",
       "      <td>to</td>\n",
       "      <td>2521</td>\n",
       "      <td>far</td>\n",
       "      <td>6.496281</td>\n",
       "      <td>[-7.53187, -7.481711, -7.5258093, -7.5237474, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>2562</td>\n",
       "      <td>keep</td>\n",
       "      <td>2185</td>\n",
       "      <td>away</td>\n",
       "      <td>6.685265</td>\n",
       "      <td>[-7.332101, -7.394904, -7.4043255, -7.4488153,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>20687</td>\n",
       "      <td>renew</td>\n",
       "      <td>20410</td>\n",
       "      <td>verify</td>\n",
       "      <td>2.409274</td>\n",
       "      <td>[-4.0348964, -4.034002, -4.3296347, -4.0737686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>[-14.3275175, -14.694819, -14.702918, -14.3263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>39</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>[-9.652537, -9.60094, -9.601458, -9.620717, -9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>He was a character.</td>\n",
       "      <td>[[CLS], he, was, a, character, ., [SEP]]</td>\n",
       "      <td>1</td>\n",
       "      <td>2002</td>\n",
       "      <td>he</td>\n",
       "      <td>2002</td>\n",
       "      <td>he</td>\n",
       "      <td>1.577117</td>\n",
       "      <td>[-5.992837, -6.0397696, -5.9919176, -5.994485,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>He was a character.</td>\n",
       "      <td>[[CLS], he, was, a, character, ., [SEP]]</td>\n",
       "      <td>4</td>\n",
       "      <td>2839</td>\n",
       "      <td>character</td>\n",
       "      <td>2839</td>\n",
       "      <td>character</td>\n",
       "      <td>0.032695</td>\n",
       "      <td>[-9.127079, -9.143774, -9.267379, -9.157009, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>He was a character.</td>\n",
       "      <td>[[CLS], he, was, a, character, ., [SEP]]</td>\n",
       "      <td>5</td>\n",
       "      <td>1012</td>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "      <td>.</td>\n",
       "      <td>0.203213</td>\n",
       "      <td>[-7.7447324, -7.534629, -7.872818, -7.649254, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>6</td>\n",
       "      <td>2087</td>\n",
       "      <td>most</td>\n",
       "      <td>2225</td>\n",
       "      <td>west</td>\n",
       "      <td>6.531352</td>\n",
       "      <td>[-8.067986, -7.9866085, -7.9007835, -7.8676376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>14</td>\n",
       "      <td>2123</td>\n",
       "      <td>don</td>\n",
       "      <td>2123</td>\n",
       "      <td>don</td>\n",
       "      <td>0.061240</td>\n",
       "      <td>[-6.1624947, -5.961982, -6.1122904, -6.1582136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>16</td>\n",
       "      <td>1056</td>\n",
       "      <td>t</td>\n",
       "      <td>1056</td>\n",
       "      <td>t</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>[-14.047754, -14.1933775, -14.06629, -13.89654...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>18</td>\n",
       "      <td>2129</td>\n",
       "      <td>how</td>\n",
       "      <td>13898</td>\n",
       "      <td>rodney</td>\n",
       "      <td>7.105103</td>\n",
       "      <td>[-7.405884, -7.4077444, -7.314245, -7.3268056,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>19</td>\n",
       "      <td>2116</td>\n",
       "      <td>many</td>\n",
       "      <td>2001</td>\n",
       "      <td>was</td>\n",
       "      <td>8.726265</td>\n",
       "      <td>[-6.1750116, -6.28487, -6.047803, -6.19385, -5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>20</td>\n",
       "      <td>5444</td>\n",
       "      <td>voted</td>\n",
       "      <td>2024</td>\n",
       "      <td>are</td>\n",
       "      <td>7.347785</td>\n",
       "      <td>[-6.666068, -6.673855, -6.6032705, -6.706757, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>38</td>\n",
       "      <td>2156</td>\n",
       "      <td>see</td>\n",
       "      <td>2003</td>\n",
       "      <td>is</td>\n",
       "      <td>2.909826</td>\n",
       "      <td>[-7.3598285, -7.4892025, -7.4325323, -7.161677...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>40</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>1.209955</td>\n",
       "      <td>[-7.239806, -7.266509, -7.0919533, -7.151936, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>53</td>\n",
       "      <td>1024</td>\n",
       "      <td>:</td>\n",
       "      <td>1012</td>\n",
       "      <td>.</td>\n",
       "      <td>4.274540</td>\n",
       "      <td>[-8.629343, -8.646674, -8.653973, -8.456076, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>I will never denounce Mulldog. [URL]</td>\n",
       "      <td>[[CLS], i, will, never, den, ##oun, ##ce, mu, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>24471</td>\n",
       "      <td>ur</td>\n",
       "      <td>1038</td>\n",
       "      <td>b</td>\n",
       "      <td>3.629005</td>\n",
       "      <td>[-5.546036, -5.6705723, -6.0880394, -5.5258126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>hey obama fans you all got cucked, once you co...</td>\n",
       "      <td>[[CLS], hey, obama, fans, you, all, got, cu, #...</td>\n",
       "      <td>10</td>\n",
       "      <td>2320</td>\n",
       "      <td>once</td>\n",
       "      <td>2065</td>\n",
       "      <td>if</td>\n",
       "      <td>5.177674</td>\n",
       "      <td>[-7.9113235, -7.7681007, -7.589546, -7.6268983...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>hey obama fans you all got cucked, once you co...</td>\n",
       "      <td>[[CLS], hey, obama, fans, you, all, got, cu, #...</td>\n",
       "      <td>14</td>\n",
       "      <td>3408</td>\n",
       "      <td>terms</td>\n",
       "      <td>3408</td>\n",
       "      <td>terms</td>\n",
       "      <td>0.145805</td>\n",
       "      <td>[-7.741465, -8.198416, -7.470524, -7.724697, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    case_id                                               text  \\\n",
       "0         0  Before that I was down to just go and vote for...   \n",
       "1         0  Before that I was down to just go and vote for...   \n",
       "2         0  Before that I was down to just go and vote for...   \n",
       "3         0  Before that I was down to just go and vote for...   \n",
       "4         0  Before that I was down to just go and vote for...   \n",
       "5         1                                He was a character.   \n",
       "6         1                                He was a character.   \n",
       "7         1                                He was a character.   \n",
       "8         2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "9         2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "10        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "11        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "12        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "13        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "14        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "15        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "16        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "17        3               I will never denounce Mulldog. [URL]   \n",
       "18        4  hey obama fans you all got cucked, once you co...   \n",
       "19        4  hey obama fans you all got cucked, once you co...   \n",
       "\n",
       "                                       tokenized_text  masked_token_array_id  \\\n",
       "0   [[CLS], before, that, i, was, down, to, just, ...                     14   \n",
       "1   [[CLS], before, that, i, was, down, to, just, ...                     15   \n",
       "2   [[CLS], before, that, i, was, down, to, just, ...                     17   \n",
       "3   [[CLS], before, that, i, was, down, to, just, ...                     26   \n",
       "4   [[CLS], before, that, i, was, down, to, just, ...                     39   \n",
       "5            [[CLS], he, was, a, character, ., [SEP]]                      1   \n",
       "6            [[CLS], he, was, a, character, ., [SEP]]                      4   \n",
       "7            [[CLS], he, was, a, character, ., [SEP]]                      5   \n",
       "8   [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                      6   \n",
       "9   [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     14   \n",
       "10  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     16   \n",
       "11  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     18   \n",
       "12  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     19   \n",
       "13  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     20   \n",
       "14  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     38   \n",
       "15  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     40   \n",
       "16  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     53   \n",
       "17  [[CLS], i, will, never, den, ##oun, ##ce, mu, ...                     12   \n",
       "18  [[CLS], hey, obama, fans, you, all, got, cu, #...                     10   \n",
       "19  [[CLS], hey, obama, fans, you, all, got, cu, #...                     14   \n",
       "\n",
       "    masked_token_vocab_id masked_token_text  top_pred_token_vocab_id  \\\n",
       "0                    2000                to                     2521   \n",
       "1                    2562              keep                     2185   \n",
       "2                   20687             renew                    20410   \n",
       "3                    1998               and                     1998   \n",
       "4                    1996               the                     1996   \n",
       "5                    2002                he                     2002   \n",
       "6                    2839         character                     2839   \n",
       "7                    1012                 .                     1012   \n",
       "8                    2087              most                     2225   \n",
       "9                    2123               don                     2123   \n",
       "10                   1056                 t                     1056   \n",
       "11                   2129               how                    13898   \n",
       "12                   2116              many                     2001   \n",
       "13                   5444             voted                     2024   \n",
       "14                   2156               see                     2003   \n",
       "15                   1996               the                     1996   \n",
       "16                   1024                 :                     1012   \n",
       "17                  24471                ur                     1038   \n",
       "18                   2320              once                     2065   \n",
       "19                   3408             terms                     3408   \n",
       "\n",
       "   top_pred_token_text   ce_loss  \\\n",
       "0                  far  6.496281   \n",
       "1                 away  6.685265   \n",
       "2               verify  2.409274   \n",
       "3                  and  0.000039   \n",
       "4                  the  0.123853   \n",
       "5                   he  1.577117   \n",
       "6            character  0.032695   \n",
       "7                    .  0.203213   \n",
       "8                 west  6.531352   \n",
       "9                  don  0.061240   \n",
       "10                   t  0.000083   \n",
       "11              rodney  7.105103   \n",
       "12                 was  8.726265   \n",
       "13                 are  7.347785   \n",
       "14                  is  2.909826   \n",
       "15                 the  1.209955   \n",
       "16                   .  4.274540   \n",
       "17                   b  3.629005   \n",
       "18                  if  5.177674   \n",
       "19               terms  0.145805   \n",
       "\n",
       "                                          pred_logits  \n",
       "0   [-7.53187, -7.481711, -7.5258093, -7.5237474, ...  \n",
       "1   [-7.332101, -7.394904, -7.4043255, -7.4488153,...  \n",
       "2   [-4.0348964, -4.034002, -4.3296347, -4.0737686...  \n",
       "3   [-14.3275175, -14.694819, -14.702918, -14.3263...  \n",
       "4   [-9.652537, -9.60094, -9.601458, -9.620717, -9...  \n",
       "5   [-5.992837, -6.0397696, -5.9919176, -5.994485,...  \n",
       "6   [-9.127079, -9.143774, -9.267379, -9.157009, -...  \n",
       "7   [-7.7447324, -7.534629, -7.872818, -7.649254, ...  \n",
       "8   [-8.067986, -7.9866085, -7.9007835, -7.8676376...  \n",
       "9   [-6.1624947, -5.961982, -6.1122904, -6.1582136...  \n",
       "10  [-14.047754, -14.1933775, -14.06629, -13.89654...  \n",
       "11  [-7.405884, -7.4077444, -7.314245, -7.3268056,...  \n",
       "12  [-6.1750116, -6.28487, -6.047803, -6.19385, -5...  \n",
       "13  [-6.666068, -6.673855, -6.6032705, -6.706757, ...  \n",
       "14  [-7.3598285, -7.4892025, -7.4325323, -7.161677...  \n",
       "15  [-7.239806, -7.266509, -7.0919533, -7.151936, ...  \n",
       "16  [-8.629343, -8.646674, -8.653973, -8.456076, -...  \n",
       "17  [-5.546036, -5.6705723, -6.0880394, -5.5258126...  \n",
       "18  [-7.9113235, -7.7681007, -7.589546, -7.6268983...  \n",
       "19  [-7.741465, -8.198416, -7.470524, -7.724697, -...  "
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-candy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
