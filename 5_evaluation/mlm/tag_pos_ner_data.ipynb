{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mexican-milan",
   "metadata": {},
   "source": [
    "# Apply POS Tagging and NER to Politics Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electrical-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from spacy.symbols import ORTH\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numerical-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model for POS tagging and NER\n",
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# custom punctuation infix handling --> square and round parentheses split words\n",
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[..\\,\\-\\[\\]\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "spacy_model.tokenizer = custom_tokenizer(spacy_model)\n",
    "\n",
    "for special_tokens in [\"[URL]\", \"[EMOJI]\"]:\n",
    "    special_case = [{ORTH: special_tokens}]\n",
    "    spacy_model.tokenizer.add_special_case(special_tokens, special_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "minor-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading test_2017_03_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_04_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_05_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_06_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_07_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_08_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_09_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_10_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_11_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2017_12_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_01_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_02_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_03_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_04_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_05_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_06_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_07_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_08_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_09_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_10_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_11_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2018_12_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_01_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_02_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_03_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_04_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_05_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_06_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_07_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_08_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_09_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_10_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_11_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2019_12_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2020_01_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "reading test_2020_02_5k.txt\n",
      "  writing POS tags\n",
      "  writing NER tags\n",
      "  writing tokenized texts\n",
      "CPU times: user 55min 23s, sys: 2min 21s, total: 57min 45s\n",
      "Wall time: 58min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spacy POS tags\n",
    "def text_to_pos_list(text):\n",
    "    doc = spacy_model(text)\n",
    "    return [[token.text, token.pos_] for token in doc]\n",
    "\n",
    "\n",
    "# spacy NER tags\n",
    "def text_to_ner_list(text):\n",
    "    doc = spacy_model(text)\n",
    "    text_ner_list = []\n",
    "    for token in doc:\n",
    "        if str(token.ent_type_)!=\"\":\n",
    "            text_ner_list.append([token.text, token.ent_type_, token.ent_iob_])\n",
    "        else: \n",
    "            text_ner_list.append([token.text, \"NO_ENT\", token.ent_iob_])\n",
    "    return text_ner_list\n",
    "\n",
    "\n",
    "def load_df_from_txt(path):\n",
    "    \n",
    "    with open(path, \"r\") as file:\n",
    "        df = pd.DataFrame(pd.Series(line.rstrip(\"\\n\") for line in file if len(line) > 0 and not line.isspace()), columns=[\"text\"])\n",
    "        \n",
    "    # write spacy POS tags and NER to new columns\n",
    "    print(\"  writing POS tags\")\n",
    "    df[\"text_pos\"] = df.text.apply(lambda x: text_to_pos_list(x))\n",
    "    print(\"  writing NER tags\")\n",
    "    df[\"text_ner\"] = df.text.apply(lambda x: text_to_ner_list(x))\n",
    "    \n",
    "    # write BERT tokenized version of input text to new column\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../../0_models/bert-rand-1m-3ep-rand', use_fast=True)\n",
    "    print(\"  writing tokenized texts\")\n",
    "    df[\"text_tokens\"] = df.text.apply(lambda x: tokenizer.convert_ids_to_tokens(tokenizer(x, truncation=True, max_length=128, return_special_tokens_mask=True)[\"input_ids\"]))\n",
    "        \n",
    "    return df\n",
    "\n",
    "test_set_dict = {}\n",
    "\n",
    "directory = '../../0_data/clean/unlabelled_reddit/politics_test'\n",
    "for filename in sorted(os.listdir(directory)):\n",
    "    if filename.endswith(\"_5k.txt\"):\n",
    "        print(f\"reading {filename}\")\n",
    "        test_set_dict[filename.rstrip(\".txt\")] = load_df_from_txt(os.path.join(directory, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reserved-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_pos_to_token(row):\n",
    "    token_pos_list = []\n",
    "    \n",
    "    token_counter = -1\n",
    "    \n",
    "    for token in row.text_tokens:\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]: # catch special tokens\n",
    "            token_pos_list.append([token, \"SPECIAL\"])\n",
    "        elif \"##\" in token: # if BERT token is continuation, don't increment spacy token counter and append POS tag equal to previous iteration\n",
    "            token_pos_list.append([token, row.text_pos[token_counter][1]])\n",
    "        else:\n",
    "            if token_counter<(len(row.text_pos)-1): # ensure that spacy token_counter does not exceed number of spacy tokens (e.g. because of ellipses)\n",
    "                token_counter+=1\n",
    "\n",
    "            if str.lower(token) in str.lower(row.text_pos[token_counter][0]) or str.lower(row.text_pos[token_counter][0]) in str.lower(token): #let incrementation happen if BERT and spacy token partially match\n",
    "                token_pos_list.append([token, row.text_pos[token_counter][1]])\n",
    "            else:\n",
    "                token_counter-=1\n",
    "                token_pos_list.append([token, row.text_pos[token_counter][1]])\n",
    "\n",
    "    return token_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "impressed-bleeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ent_to_token(row):\n",
    "    token_ent_list = []\n",
    "    \n",
    "    token_counter = -1\n",
    "    \n",
    "    for token in row.text_tokens:\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]: # catch special tokens\n",
    "            token_ent_list.append([token, \"SPECIAL\"])\n",
    "        elif \"##\" in token: # if BERT token is continuation, don't increment spacy token counter and append POS tag equal to previous iteration\n",
    "            token_ent_list.append([token, row.text_ner[token_counter][1]])\n",
    "        else:\n",
    "            if token_counter<(len(row.text_ner)-1): # ensure that spacy token_counter does not exceed number of spacy tokens (e.g. because of ellipses)\n",
    "                token_counter+=1\n",
    "\n",
    "            if str.lower(token) in str.lower(row.text_ner[token_counter][0]) or str.lower(row.text_ner[token_counter][0]) in str.lower(token): #let incrementation happen if BERT and spacy token partially match\n",
    "                token_ent_list.append([token, row.text_ner[token_counter][1]])\n",
    "            else:\n",
    "                token_counter-=1\n",
    "                token_ent_list.append([token, row.text_ner[token_counter][1]])\n",
    "\n",
    "    return token_ent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "referenced-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2017_03_5k\n",
      "test_2017_04_5k\n",
      "test_2017_05_5k\n",
      "test_2017_06_5k\n",
      "test_2017_07_5k\n",
      "test_2017_08_5k\n",
      "test_2017_09_5k\n",
      "test_2017_10_5k\n",
      "test_2017_11_5k\n",
      "test_2017_12_5k\n",
      "test_2018_01_5k\n",
      "test_2018_02_5k\n",
      "test_2018_03_5k\n",
      "test_2018_04_5k\n",
      "test_2018_05_5k\n",
      "test_2018_06_5k\n",
      "test_2018_07_5k\n",
      "test_2018_08_5k\n",
      "test_2018_09_5k\n",
      "test_2018_10_5k\n",
      "test_2018_11_5k\n",
      "test_2018_12_5k\n",
      "test_2019_01_5k\n",
      "test_2019_02_5k\n",
      "test_2019_03_5k\n",
      "test_2019_04_5k\n",
      "test_2019_05_5k\n",
      "test_2019_06_5k\n",
      "test_2019_07_5k\n",
      "test_2019_08_5k\n",
      "test_2019_09_5k\n",
      "test_2019_10_5k\n",
      "test_2019_11_5k\n",
      "test_2019_12_5k\n",
      "test_2020_01_5k\n",
      "test_2020_02_5k\n",
      "CPU times: user 2min 18s, sys: 4.42 s, total: 2min 22s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# assign spacy POS tags to BERT subword tokens\n",
    "for test_set in test_set_dict:\n",
    "    print(test_set)\n",
    "    test_set_dict[test_set][\"tokens_pos\"] =  test_set_dict[test_set].apply(lambda x: map_pos_to_token(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "governing-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2017_03_5k\n",
      "test_2017_04_5k\n",
      "test_2017_05_5k\n",
      "test_2017_06_5k\n",
      "test_2017_07_5k\n",
      "test_2017_08_5k\n",
      "test_2017_09_5k\n",
      "test_2017_10_5k\n",
      "test_2017_11_5k\n",
      "test_2017_12_5k\n",
      "test_2018_01_5k\n",
      "test_2018_02_5k\n",
      "test_2018_03_5k\n",
      "test_2018_04_5k\n",
      "test_2018_05_5k\n",
      "test_2018_06_5k\n",
      "test_2018_07_5k\n",
      "test_2018_08_5k\n",
      "test_2018_09_5k\n",
      "test_2018_10_5k\n",
      "test_2018_11_5k\n",
      "test_2018_12_5k\n",
      "test_2019_01_5k\n",
      "test_2019_02_5k\n",
      "test_2019_03_5k\n",
      "test_2019_04_5k\n",
      "test_2019_05_5k\n",
      "test_2019_06_5k\n",
      "test_2019_07_5k\n",
      "test_2019_08_5k\n",
      "test_2019_09_5k\n",
      "test_2019_10_5k\n",
      "test_2019_11_5k\n",
      "test_2019_12_5k\n",
      "test_2020_01_5k\n",
      "test_2020_02_5k\n",
      "CPU times: user 2min 17s, sys: 5.39 s, total: 2min 22s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# assign spacy ENT tags to BERT subword tokens\n",
    "for test_set in test_set_dict:\n",
    "    print(test_set)\n",
    "    test_set_dict[test_set][\"tokens_ner\"] =  test_set_dict[test_set].apply(lambda x: map_ent_to_token(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "increasing-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to to csv\n",
    "for test_set in test_set_dict:\n",
    "    test_set_dict[test_set].to_csv(f\"../../0_data/clean/unlabelled_reddit/error_analysis/pol_{test_set}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-community",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
