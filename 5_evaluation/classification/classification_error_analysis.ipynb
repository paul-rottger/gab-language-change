{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "touched-isolation",
   "metadata": {},
   "source": [
    "# EVALUATION OF PROPN FREQUENCY MEASURES IN PSP TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sapphire-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from ast import literal_eval\n",
    "from scipy.stats import spearmanr, ttest_ind, pearsonr\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-honor",
   "metadata": {},
   "source": [
    "### Import Data for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "express-victor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pol_test_2017_03_5k.csv token set\n",
      "loading pol_test_2017_04_5k.csv token set\n",
      "loading pol_test_2017_05_5k.csv token set\n",
      "loading pol_test_2017_06_5k.csv token set\n",
      "loading pol_test_2017_07_5k.csv token set\n",
      "loading pol_test_2017_08_5k.csv token set\n",
      "loading pol_test_2017_09_5k.csv token set\n",
      "loading pol_test_2017_10_5k.csv token set\n",
      "loading pol_test_2017_11_5k.csv token set\n",
      "loading pol_test_2017_12_5k.csv token set\n",
      "loading pol_test_2018_01_5k.csv token set\n",
      "loading pol_test_2018_02_5k.csv token set\n",
      "loading pol_test_2018_03_5k.csv token set\n",
      "loading pol_test_2018_04_5k.csv token set\n",
      "loading pol_test_2018_05_5k.csv token set\n",
      "loading pol_test_2018_06_5k.csv token set\n",
      "loading pol_test_2018_07_5k.csv token set\n",
      "loading pol_test_2018_08_5k.csv token set\n",
      "loading pol_test_2018_09_5k.csv token set\n",
      "loading pol_test_2018_10_5k.csv token set\n",
      "loading pol_test_2018_11_5k.csv token set\n",
      "loading pol_test_2018_12_5k.csv token set\n",
      "loading pol_test_2019_01_5k.csv token set\n",
      "loading pol_test_2019_02_5k.csv token set\n",
      "loading pol_test_2019_03_5k.csv token set\n",
      "loading pol_test_2019_04_5k.csv token set\n",
      "loading pol_test_2019_05_5k.csv token set\n",
      "loading pol_test_2019_06_5k.csv token set\n",
      "loading pol_test_2019_07_5k.csv token set\n",
      "loading pol_test_2019_08_5k.csv token set\n",
      "loading pol_test_2019_09_5k.csv token set\n",
      "loading pol_test_2019_10_5k.csv token set\n",
      "loading pol_test_2019_11_5k.csv token set\n",
      "loading pol_test_2019_12_5k.csv token set\n",
      "loading pol_test_2020_01_5k.csv token set\n",
      "loading pol_test_2020_02_5k.csv token set\n",
      "CPU times: user 1min 52s, sys: 10.8 s, total: 2min 2s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load token breakdown of test documents\n",
    "\n",
    "full_df_dict = {}\n",
    "\n",
    "directory = f'../../0_data/clean/unlabelled_reddit/error_analysis'\n",
    "\n",
    "for csv in sorted(os.listdir(directory)):\n",
    "    if csv.endswith(\"_5k.csv\"):\n",
    "        print(f\"loading {csv} token set\")\n",
    "        full_df_dict[os.path.splitext(csv)[0].lstrip(\"pol_\")] = pd.read_csv(os.path.join(directory, csv),\n",
    "                                                                            converters={\"tokens_pos\": literal_eval, 'text_pos': literal_eval})[[\"text\", \"text_pos\", \"tokens_pos\"]]\n",
    "        full_df_dict[os.path.splitext(csv)[0].lstrip(\"pol_\")].index = full_df_dict[os.path.splitext(csv)[0].lstrip(\"pol_\")].sample(frac=1, random_state=123).index\n",
    "        full_df_dict[os.path.splitext(csv)[0].lstrip(\"pol_\")].sort_index(inplace=True)\n",
    "\n",
    "\n",
    "# load document-level MLM CE loss and merge with test set DFs\n",
    "\n",
    "directory = f'../../0_data/clean/labelled_reddit/error_analysis'\n",
    "\n",
    "for csv in os.listdir(directory):\n",
    "    for key in full_df_dict.keys():\n",
    "        if os.path.splitext(csv)[0] == key: # only load and merge matching test set\n",
    "            full_df_dict[key] = full_df_dict[key].merge(pd.read_csv(os.path.join(directory, csv))[[\"label\", \"ce_diff_base_rand\", \"ce_diff_rand_match\"]],\n",
    "                                                        left_index=True, right_index=True)\n",
    "            \n",
    "            \n",
    "id_to_label = {\n",
    "    0: \"ChapoTrapHouse\",\n",
    "    1: \"Conservative\",\n",
    "    2: \"Libertarian\",\n",
    "    3: \"The_Donald\",\n",
    "    4: \"politics\"\n",
    "}\n",
    "            \n",
    "# load document-level prediction results for different model types\n",
    "for model, name in [(\"base+month\", \"base\"), (\"rand+month\", \"rand\"),  (\"month+month\", \"match\")]:\n",
    "    \n",
    "    directory = f\"../../0_results/classification/reddit/month-models/{model}\"\n",
    "    \n",
    "    for csv in os.listdir(directory):\n",
    "        \n",
    "        for key in full_df_dict.keys():\n",
    "            \n",
    "            if (key in csv) and (\"train_\"+re.search(\"test_(.*?)_5k\", key).group(1) in csv) and (\"_20k-test\" in csv):\n",
    "                \n",
    "                in_df = pd.read_csv(os.path.join(directory, csv))[[\"index\", \"prediction\"]].rename(columns={\"prediction\": f\"pred_{name}\"}).set_index(\"index\")\n",
    "                \n",
    "                for k_id in id_to_label:\n",
    "                    in_df[f\"pred_{name}\"].replace(k_id, id_to_label[k_id], inplace=True)\n",
    "                    \n",
    "                full_df_dict[key] = full_df_dict[key].merge(in_df, left_index=True, right_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-injection",
   "metadata": {},
   "source": [
    "### Concatenate Monthly Sets to Full DF for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "knowing-constitutional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 209 ms, sys: 145 ms, total: 353 ms\n",
      "Wall time: 386 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_pos</th>\n",
       "      <th>tokens_pos</th>\n",
       "      <th>label</th>\n",
       "      <th>ce_diff_base_rand</th>\n",
       "      <th>ce_diff_rand_match</th>\n",
       "      <th>pred_base</th>\n",
       "      <th>pred_rand</th>\n",
       "      <th>pred_match</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice argument there twinky. Tell your mom I sa...</td>\n",
       "      <td>[[Nice, ADJ], [argument, NOUN], [there, ADV], ...</td>\n",
       "      <td>[[[CLS], SPECIAL], [nice, ADJ], [argument, NOU...</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>2.450879</td>\n",
       "      <td>0.389335</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>politics</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>test_2017_03_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes, why should the rest of the world be entit...</td>\n",
       "      <td>[[Yes, INTJ], [,, PUNCT], [why, ADV], [should,...</td>\n",
       "      <td>[[[CLS], SPECIAL], [yes, INTJ], [,, PUNCT], [w...</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>-0.073872</td>\n",
       "      <td>0.014518</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>test_2017_03_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Definitely. They are getting plenty of corpora...</td>\n",
       "      <td>[[Definitely, ADV], [., PUNCT], [They, PRON], ...</td>\n",
       "      <td>[[[CLS], SPECIAL], [definitely, ADV], [., PUNC...</td>\n",
       "      <td>politics</td>\n",
       "      <td>9.096548</td>\n",
       "      <td>1.122668</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>test_2017_03_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Their data is protected as \"national security\"...</td>\n",
       "      <td>[[Their, PRON], [data, NOUN], [is, AUX], [prot...</td>\n",
       "      <td>[[[CLS], SPECIAL], [their, PRON], [data, NOUN]...</td>\n",
       "      <td>politics</td>\n",
       "      <td>0.090673</td>\n",
       "      <td>-1.162980</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>Libertarian</td>\n",
       "      <td>test_2017_03_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>they posted a pic months ago from a \"white hou...</td>\n",
       "      <td>[[they, PRON], [posted, VERB], [a, DET], [pic,...</td>\n",
       "      <td>[[[CLS], SPECIAL], [they, PRON], [posted, VERB...</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>1.158620</td>\n",
       "      <td>1.648456</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>politics</td>\n",
       "      <td>test_2017_03_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179995</th>\n",
       "      <td>I’d be disappointed in them if they didn’t. /s</td>\n",
       "      <td>[[I, PRON], [’, VERB], [d, X], [be, AUX], [dis...</td>\n",
       "      <td>[[[CLS], SPECIAL], [i, PRON], [’, VERB], [d, X...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>51.887914</td>\n",
       "      <td>0.114969</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>test_2020_02_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179996</th>\n",
       "      <td>Bernie has a wider margin against Trump when y...</td>\n",
       "      <td>[[Bernie, PROPN], [has, VERB], [a, DET], [wide...</td>\n",
       "      <td>[[[CLS], SPECIAL], [bernie, PROPN], [has, VERB...</td>\n",
       "      <td>politics</td>\n",
       "      <td>5.673599</td>\n",
       "      <td>3.070925</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>politics</td>\n",
       "      <td>test_2020_02_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179997</th>\n",
       "      <td>I just wish Chelsea Handler would wind up as a...</td>\n",
       "      <td>[[I, PRON], [just, ADV], [wish, VERB], [Chelse...</td>\n",
       "      <td>[[[CLS], SPECIAL], [i, PRON], [just, ADV], [wi...</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>7.047268</td>\n",
       "      <td>2.106946</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>ChapoTrapHouse</td>\n",
       "      <td>test_2020_02_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179998</th>\n",
       "      <td>NO major newspaper coverage. [twatter (sic) li...</td>\n",
       "      <td>[[NO, DET], [major, ADJ], [newspaper, NOUN], [...</td>\n",
       "      <td>[[[CLS], SPECIAL], [no, DET], [major, ADJ], [n...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>14.046416</td>\n",
       "      <td>-1.121283</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>test_2020_02_5k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179999</th>\n",
       "      <td>It’s *culture!* You know, like white people de...</td>\n",
       "      <td>[[It, PRON], [’s, VERB], [*, NOUN], [culture, ...</td>\n",
       "      <td>[[[CLS], SPECIAL], [it, PRON], [’, VERB], [s, ...</td>\n",
       "      <td>The_Donald</td>\n",
       "      <td>18.705396</td>\n",
       "      <td>-1.114670</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>Conservative</td>\n",
       "      <td>test_2020_02_5k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Nice argument there twinky. Tell your mom I sa...   \n",
       "1       Yes, why should the rest of the world be entit...   \n",
       "2       Definitely. They are getting plenty of corpora...   \n",
       "3       Their data is protected as \"national security\"...   \n",
       "4       they posted a pic months ago from a \"white hou...   \n",
       "...                                                   ...   \n",
       "179995     I’d be disappointed in them if they didn’t. /s   \n",
       "179996  Bernie has a wider margin against Trump when y...   \n",
       "179997  I just wish Chelsea Handler would wind up as a...   \n",
       "179998  NO major newspaper coverage. [twatter (sic) li...   \n",
       "179999  It’s *culture!* You know, like white people de...   \n",
       "\n",
       "                                                 text_pos  \\\n",
       "0       [[Nice, ADJ], [argument, NOUN], [there, ADV], ...   \n",
       "1       [[Yes, INTJ], [,, PUNCT], [why, ADV], [should,...   \n",
       "2       [[Definitely, ADV], [., PUNCT], [They, PRON], ...   \n",
       "3       [[Their, PRON], [data, NOUN], [is, AUX], [prot...   \n",
       "4       [[they, PRON], [posted, VERB], [a, DET], [pic,...   \n",
       "...                                                   ...   \n",
       "179995  [[I, PRON], [’, VERB], [d, X], [be, AUX], [dis...   \n",
       "179996  [[Bernie, PROPN], [has, VERB], [a, DET], [wide...   \n",
       "179997  [[I, PRON], [just, ADV], [wish, VERB], [Chelse...   \n",
       "179998  [[NO, DET], [major, ADJ], [newspaper, NOUN], [...   \n",
       "179999  [[It, PRON], [’s, VERB], [*, NOUN], [culture, ...   \n",
       "\n",
       "                                               tokens_pos           label  \\\n",
       "0       [[[CLS], SPECIAL], [nice, ADJ], [argument, NOU...     Libertarian   \n",
       "1       [[[CLS], SPECIAL], [yes, INTJ], [,, PUNCT], [w...     Libertarian   \n",
       "2       [[[CLS], SPECIAL], [definitely, ADV], [., PUNC...        politics   \n",
       "3       [[[CLS], SPECIAL], [their, PRON], [data, NOUN]...        politics   \n",
       "4       [[[CLS], SPECIAL], [they, PRON], [posted, VERB...  ChapoTrapHouse   \n",
       "...                                                   ...             ...   \n",
       "179995  [[[CLS], SPECIAL], [i, PRON], [’, VERB], [d, X...    Conservative   \n",
       "179996  [[[CLS], SPECIAL], [bernie, PROPN], [has, VERB...        politics   \n",
       "179997  [[[CLS], SPECIAL], [i, PRON], [just, ADV], [wi...    Conservative   \n",
       "179998  [[[CLS], SPECIAL], [no, DET], [major, ADJ], [n...      The_Donald   \n",
       "179999  [[[CLS], SPECIAL], [it, PRON], [’, VERB], [s, ...      The_Donald   \n",
       "\n",
       "        ce_diff_base_rand  ce_diff_rand_match     pred_base       pred_rand  \\\n",
       "0                2.450879            0.389335   Libertarian        politics   \n",
       "1               -0.073872            0.014518   Libertarian     Libertarian   \n",
       "2                9.096548            1.122668      politics        politics   \n",
       "3                0.090673           -1.162980   Libertarian     Libertarian   \n",
       "4                1.158620            1.648456    The_Donald  ChapoTrapHouse   \n",
       "...                   ...                 ...           ...             ...   \n",
       "179995          51.887914            0.114969  Conservative    Conservative   \n",
       "179996           5.673599            3.070925      politics        politics   \n",
       "179997           7.047268            2.106946    The_Donald      The_Donald   \n",
       "179998          14.046416           -1.121283    The_Donald      The_Donald   \n",
       "179999          18.705396           -1.114670  Conservative    Conservative   \n",
       "\n",
       "            pred_match           source  \n",
       "0          Libertarian  test_2017_03_5k  \n",
       "1          Libertarian  test_2017_03_5k  \n",
       "2             politics  test_2017_03_5k  \n",
       "3          Libertarian  test_2017_03_5k  \n",
       "4             politics  test_2017_03_5k  \n",
       "...                ...              ...  \n",
       "179995    Conservative  test_2020_02_5k  \n",
       "179996        politics  test_2020_02_5k  \n",
       "179997  ChapoTrapHouse  test_2020_02_5k  \n",
       "179998      The_Donald  test_2020_02_5k  \n",
       "179999    Conservative  test_2020_02_5k  \n",
       "\n",
       "[180000 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# write source column to each df in dict, then concatenate all dfs into one overall df for analysis\n",
    "for testset in full_df_dict:\n",
    "    full_df_dict[testset][\"source\"] = testset\n",
    "    \n",
    "overall_df = pd.concat(full_df_dict.values(), ignore_index=True)\n",
    "overall_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-porcelain",
   "metadata": {},
   "source": [
    "### Set up Counter() dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "simplified-bangladesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 11.5 s, total: 1min 39s\n",
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create PROPN counters for each month and subreddit\n",
    "propn_counter = {}\n",
    "for month in full_df_dict:\n",
    "    propn_counter[month] = {}\n",
    "    for label in ['Libertarian', 'politics', 'ChapoTrapHouse', 'Conservative', 'The_Donald']:\n",
    "        propn_counter[month][label] = Counter()\n",
    "        for _, row in full_df_dict[month].iterrows():\n",
    "            if row.label==label:\n",
    "                previous_elems=[]\n",
    "                for elem in row.tokens_pos:\n",
    "                    if elem[1] == \"PROPN\":\n",
    "                        if elem[0] not in previous_elems:  # count only first occurence --> how many docs rather than how many tokens\n",
    "                            propn_counter[month][label][elem[0]] +=1\n",
    "                            previous_elems.append(elem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "related-cannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 565 ms, total: 1.69 s\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create overall counters for each month\n",
    "for month in propn_counter:\n",
    "    propn_counter[month][\"total\"]=sum(propn_counter[month].values(), Counter())\n",
    "\n",
    "# create overall counter across all months\n",
    "overall_counter = {}\n",
    "for label in ['Libertarian', 'politics', 'ChapoTrapHouse', 'Conservative', 'The_Donald', \"total\"]:\n",
    "    overall_counter[label] = sum([propn_counter[month][label] for month in propn_counter], Counter())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-dressing",
   "metadata": {},
   "source": [
    "### Analysis of most-improved PROPNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "smoking-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import PROPN tokens with source month\n",
    "import_df = pd.read_csv(\"../../0_data/clean/labelled_reddit/error_analysis/most_improved_propn.csv\")\n",
    "\n",
    "# select top N%\n",
    "N=10\n",
    "most_improved_df = import_df.head(int(import_df.shape[0]*N/100)).copy()\n",
    "most_improved_df.drop_duplicates(subset=[\"masked_token_text\", \"source\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "suited-pointer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 769 ms, sys: 6.16 ms, total: 775 ms\n",
      "Wall time: 779 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# how many subreddits did they appear in?\n",
    "\n",
    "def count_subs(row):\n",
    "    counter = 0\n",
    "    for sub in ['Libertarian', 'politics', 'ChapoTrapHouse', 'Conservative', 'The_Donald']:\n",
    "        if propn_counter[row.source][sub][row.masked_token_text]>0:\n",
    "            counter+=1\n",
    "    return counter\n",
    "\n",
    "def count_total(row):\n",
    "    return propn_counter[row.source][\"total\"][row.masked_token_text]\n",
    "\n",
    "def max_count_single_sub(row):\n",
    "    counter = 0\n",
    "    for sub in ['Libertarian', 'politics', 'ChapoTrapHouse', 'Conservative', 'The_Donald']:\n",
    "        if propn_counter[row.source][sub][row.masked_token_text]>counter:\n",
    "            counter = propn_counter[row.source][sub][row.masked_token_text]\n",
    "    return counter\n",
    "\n",
    "most_improved_df[\"n_sub_with_occurrence\"] = most_improved_df.apply(lambda x: count_subs(x), axis=1)\n",
    "most_improved_df[\"n_total\"] = most_improved_df.apply(lambda x: count_total(x), axis=1)\n",
    "most_improved_df[\"n_max_sub\"] = most_improved_df.apply(lambda x: max_count_single_sub(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "unnecessary-coordination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_7c5ae_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >n</th>        <th class=\"col_heading level0 col1\" >n_sub_with_occurrence</th>        <th class=\"col_heading level0 col2\" >n_total_x</th>        <th class=\"col_heading level0 col3\" >n_sub_average</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_7c5ae_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_7c5ae_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "                        <td id=\"T_7c5ae_row0_col1\" class=\"data row0 col1\" >1403</td>\n",
       "                        <td id=\"T_7c5ae_row0_col2\" class=\"data row0 col2\" >1789</td>\n",
       "                        <td id=\"T_7c5ae_row0_col3\" class=\"data row0 col3\" >1.28</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7c5ae_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_7c5ae_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "                        <td id=\"T_7c5ae_row1_col1\" class=\"data row1 col1\" >769</td>\n",
       "                        <td id=\"T_7c5ae_row1_col2\" class=\"data row1 col2\" >2316</td>\n",
       "                        <td id=\"T_7c5ae_row1_col3\" class=\"data row1 col3\" >1.51</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7c5ae_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_7c5ae_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "                        <td id=\"T_7c5ae_row2_col1\" class=\"data row2 col1\" >559</td>\n",
       "                        <td id=\"T_7c5ae_row2_col2\" class=\"data row2 col2\" >3336</td>\n",
       "                        <td id=\"T_7c5ae_row2_col3\" class=\"data row2 col3\" >1.99</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7c5ae_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_7c5ae_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "                        <td id=\"T_7c5ae_row3_col1\" class=\"data row3 col1\" >570</td>\n",
       "                        <td id=\"T_7c5ae_row3_col2\" class=\"data row3 col2\" >6950</td>\n",
       "                        <td id=\"T_7c5ae_row3_col3\" class=\"data row3 col3\" >3.05</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_7c5ae_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_7c5ae_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "                        <td id=\"T_7c5ae_row4_col1\" class=\"data row4 col1\" >819</td>\n",
       "                        <td id=\"T_7c5ae_row4_col2\" class=\"data row4 col2\" >33090</td>\n",
       "                        <td id=\"T_7c5ae_row4_col3\" class=\"data row4 col3\" >8.08</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8d916e0970>"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_df = pd.DataFrame(most_improved_df.groupby(\"n_sub_with_occurrence\").n_sub_with_occurrence.count())\n",
    "display_df = display_df.merge(most_improved_df.groupby(\"n_sub_with_occurrence\").n_total.sum(), left_index=True, right_index=True)\n",
    "display_df = display_df.merge(most_improved_df.groupby(\"n_sub_with_occurrence\").n_total.mean(), left_index=True, right_index=True)\n",
    "display_df = display_df.merge(most_improved_df.groupby(\"n_sub_with_occurrence\").n_max_sub.mean(), left_index=True, right_index=True)\n",
    "display_df.index.name = \"n\"\n",
    "display_df.reset_index(inplace=True)\n",
    "display_df[\"n_sub_average\"] = display_df.n_total_y/display_df.n\n",
    "display_df[[\"n\", \"n_sub_with_occurrence\", \"n_total_x\", \"n_sub_average\"]].style.set_precision(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "appreciated-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libertarian 9\n",
      "politics 17\n",
      "ChapoTrapHouse 2\n",
      "Conservative 28\n",
      "The_Donald 11\n",
      "total 67\n"
     ]
    }
   ],
   "source": [
    "# frequency of particular token in particular month across subreddits\n",
    "\n",
    "word = \"##ugh\"\n",
    "month = \"test_2018_10_5k\"\n",
    "for sub in propn_counter[month]:\n",
    "    print(sub, propn_counter[month][sub][word] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "corporate-boulder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2017_03_5k 0\n",
      "test_2017_04_5k 1\n",
      "test_2017_05_5k 0\n",
      "test_2017_06_5k 0\n",
      "test_2017_07_5k 0\n",
      "test_2017_08_5k 0\n",
      "test_2017_09_5k 0\n",
      "test_2017_10_5k 0\n",
      "test_2017_11_5k 1\n",
      "test_2017_12_5k 0\n",
      "test_2018_01_5k 0\n",
      "test_2018_02_5k 0\n",
      "test_2018_03_5k 0\n",
      "test_2018_04_5k 1\n",
      "test_2018_05_5k 0\n",
      "test_2018_06_5k 2\n",
      "test_2018_07_5k 9\n",
      "test_2018_08_5k 1\n",
      "test_2018_09_5k 107\n",
      "test_2018_10_5k 67\n",
      "test_2018_11_5k 11\n",
      "test_2018_12_5k 6\n",
      "test_2019_01_5k 7\n",
      "test_2019_02_5k 6\n",
      "test_2019_03_5k 9\n",
      "test_2019_04_5k 3\n",
      "test_2019_05_5k 4\n",
      "test_2019_06_5k 3\n",
      "test_2019_07_5k 4\n",
      "test_2019_08_5k 2\n",
      "test_2019_09_5k 8\n",
      "test_2019_10_5k 1\n",
      "test_2019_11_5k 6\n",
      "test_2019_12_5k 4\n",
      "test_2020_01_5k 1\n",
      "test_2020_02_5k 0\n"
     ]
    }
   ],
   "source": [
    "# frequency of particular token across months\n",
    "\n",
    "word = \"##ugh\"\n",
    "for month in propn_counter:\n",
    "    print(month, propn_counter[month][\"total\"][word])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
