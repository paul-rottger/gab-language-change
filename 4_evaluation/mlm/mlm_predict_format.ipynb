{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "configured-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import Trainer, AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "emerging-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fossil-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "interesting-savings",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../0_models/default-model were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained('../../0_models/default-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "inner-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../../0_models/default-model', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "blind-ground",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fdca24a232e41a05\n",
      "Reusing dataset text (/Users/Paul/.cache/huggingface/datasets/text/default-fdca24a232e41a05/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691)\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset('text', data_files={'validation': '../../0_data/clean/unlabelled_reddit/politics_test/test_2017_03_5k.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "dominican-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/Paul/.cache/huggingface/datasets/text/default-fdca24a232e41a05/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691/cache-eb713f96e0f96ba1.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Remove empty lines\n",
    "    examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
    "    \n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=64,\n",
    "        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "advanced-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "plain-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "considered-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "rotary-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " pred done\n",
      "1\n",
      " pred done\n",
      "2\n",
      " pred done\n",
      "3\n",
      " pred done\n",
      "4\n",
      " pred done\n",
      "5\n",
      " pred done\n",
      "6\n",
      " pred done\n",
      "7\n",
      " pred done\n",
      "8\n",
      " pred done\n",
      "9\n",
      " pred done\n",
      "10\n",
      " pred done\n",
      "11\n",
      " pred done\n",
      "12\n",
      " pred done\n",
      "13\n",
      " pred done\n",
      "14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/language_change/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1715\u001b[0;31m         output = self.prediction_loop(\n\u001b[0m\u001b[1;32m   1716\u001b[0m             \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m         )\n",
      "\u001b[0;32m~/opt/miniconda3/envs/language_change/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mlosses_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlosses_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mpreds_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreds_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mlabels_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlabels_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/language_change/lib/python3.8/site-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/language_change/lib/python3.8/site-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;34m\"\"\"Concatenates `tensor1` and `tensor2` on first axis, applying padding on the second if necessary.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Let's figure out the new shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialise dictionary for writing prediction results to\n",
    "out_dict = {\"case_id\": [],\n",
    "            \"masked_token_array_id\": [], \"masked_token_vocab_id\": [], \"masked_token_text\": [],\n",
    "            \"top_pred_token_vocab_id\": [], \"top_pred_token_text\": [],\n",
    "            \"ce_loss\": [],\n",
    "            \"pred_logits\": []}\n",
    "\n",
    "# set number of shards for splitting dataset into\n",
    "n_shards=100\n",
    "\n",
    "for shard_id in range(n_shards):\n",
    "    \n",
    "    print(shard_id)\n",
    "    \n",
    "    # run prediction on shards of overall test set so as not to exceed RAM\n",
    "    test_shard = tokenized_datasets[\"validation\"].shard(n_shards, shard_id, contiguous=True)\n",
    "    pred_results = trainer.predict(test_shard)\n",
    "    \n",
    "    print(\" pred done\")\n",
    "    \n",
    "    # each row corresponds to a masked token\n",
    "    # first level of iteration is case-by-case\n",
    "    case_id_range = range(shard_id*int((tokenized_datasets[\"validation\"].shape[0]/n_shards)), (shard_id+1)*int((tokenized_datasets[\"validation\"].shape[0]/n_shards)))\n",
    "    \n",
    "    for case_id, result, label_ids in zip(case_id_range, pred_results.predictions, pred_results.label_ids):\n",
    "        \n",
    "        # second level of iteration is over masked tokens in a given case    \n",
    "        # not every case necessarily has masked tokens (indicated by label_id not equal to -100)\n",
    "        for masked_token in (label_ids != -100).nonzero()[0]:\n",
    "            \n",
    "            # write case_id, text and tokenized text corresponding to a given masked token\n",
    "            out_dict[\"case_id\"].append(case_id)\n",
    "\n",
    "            # for each masked token, write out its array id within the text, its vocab id and corresponding text\n",
    "            out_dict[\"masked_token_array_id\"].append(masked_token)\n",
    "            out_dict[\"masked_token_vocab_id\"].append(label_ids[masked_token])\n",
    "            out_dict[\"masked_token_text\"].append(tokenizer.convert_ids_to_tokens([label_ids[masked_token]])[0])\n",
    "\n",
    "            # also write the vocab id and text of the top predicted token\n",
    "            out_dict[\"top_pred_token_vocab_id\"].append(result[masked_token].argmax())\n",
    "            out_dict[\"top_pred_token_text\"].append(tokenizer.convert_ids_to_tokens([result[masked_token].argmax()])[0])\n",
    "\n",
    "            # calculate categorical cross entropy loss as the negative log of the softmax probability of the correct token\n",
    "            ce_loss = -np.log(softmax(result[masked_token])[label_ids[masked_token]])\n",
    "            out_dict[\"ce_loss\"].append(ce_loss)\n",
    "\n",
    "            # save full logits (1xvocab_size) for the masked token for flexibility in further analysis\n",
    "            out_dict[\"pred_logits\"].append(result[masked_token])\n",
    "    \n",
    "# write dataframe from dict    \n",
    "out_df = pd.DataFrame.from_dict(out_dict)\n",
    "out_df\n",
    "\n",
    "# write dataframe to csv\n",
    "#out_df.to_csv(\"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "numerical-round",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>masked_token_array_id</th>\n",
       "      <th>masked_token_vocab_id</th>\n",
       "      <th>masked_token_text</th>\n",
       "      <th>top_pred_token_vocab_id</th>\n",
       "      <th>top_pred_token_text</th>\n",
       "      <th>ce_loss</th>\n",
       "      <th>pred_logits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>14</td>\n",
       "      <td>2000</td>\n",
       "      <td>to</td>\n",
       "      <td>2521</td>\n",
       "      <td>far</td>\n",
       "      <td>6.496281</td>\n",
       "      <td>[-7.53187, -7.481711, -7.5258093, -7.5237474, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>2562</td>\n",
       "      <td>keep</td>\n",
       "      <td>2185</td>\n",
       "      <td>away</td>\n",
       "      <td>6.685265</td>\n",
       "      <td>[-7.332101, -7.394904, -7.4043255, -7.4488153,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>20687</td>\n",
       "      <td>renew</td>\n",
       "      <td>20410</td>\n",
       "      <td>verify</td>\n",
       "      <td>2.409274</td>\n",
       "      <td>[-4.0348964, -4.034002, -4.3296347, -4.0737686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>26</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "      <td>and</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>[-14.3275175, -14.694819, -14.702918, -14.3263...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Before that I was down to just go and vote for...</td>\n",
       "      <td>[[CLS], before, that, i, was, down, to, just, ...</td>\n",
       "      <td>39</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>0.123853</td>\n",
       "      <td>[-9.652537, -9.60094, -9.601458, -9.620717, -9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>He was a character.</td>\n",
       "      <td>[[CLS], he, was, a, character, ., [SEP]]</td>\n",
       "      <td>1</td>\n",
       "      <td>2002</td>\n",
       "      <td>he</td>\n",
       "      <td>2002</td>\n",
       "      <td>he</td>\n",
       "      <td>1.577117</td>\n",
       "      <td>[-5.992837, -6.0397696, -5.9919176, -5.994485,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>He was a character.</td>\n",
       "      <td>[[CLS], he, was, a, character, ., [SEP]]</td>\n",
       "      <td>4</td>\n",
       "      <td>2839</td>\n",
       "      <td>character</td>\n",
       "      <td>2839</td>\n",
       "      <td>character</td>\n",
       "      <td>0.032695</td>\n",
       "      <td>[-9.127079, -9.143774, -9.267379, -9.157009, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>He was a character.</td>\n",
       "      <td>[[CLS], he, was, a, character, ., [SEP]]</td>\n",
       "      <td>5</td>\n",
       "      <td>1012</td>\n",
       "      <td>.</td>\n",
       "      <td>1012</td>\n",
       "      <td>.</td>\n",
       "      <td>0.203213</td>\n",
       "      <td>[-7.7447324, -7.534629, -7.872818, -7.649254, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>6</td>\n",
       "      <td>2087</td>\n",
       "      <td>most</td>\n",
       "      <td>2225</td>\n",
       "      <td>west</td>\n",
       "      <td>6.531352</td>\n",
       "      <td>[-8.067986, -7.9866085, -7.9007835, -7.8676376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>14</td>\n",
       "      <td>2123</td>\n",
       "      <td>don</td>\n",
       "      <td>2123</td>\n",
       "      <td>don</td>\n",
       "      <td>0.061240</td>\n",
       "      <td>[-6.1624947, -5.961982, -6.1122904, -6.1582136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>16</td>\n",
       "      <td>1056</td>\n",
       "      <td>t</td>\n",
       "      <td>1056</td>\n",
       "      <td>t</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>[-14.047754, -14.1933775, -14.06629, -13.89654...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>18</td>\n",
       "      <td>2129</td>\n",
       "      <td>how</td>\n",
       "      <td>13898</td>\n",
       "      <td>rodney</td>\n",
       "      <td>7.105103</td>\n",
       "      <td>[-7.405884, -7.4077444, -7.314245, -7.3268056,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>19</td>\n",
       "      <td>2116</td>\n",
       "      <td>many</td>\n",
       "      <td>2001</td>\n",
       "      <td>was</td>\n",
       "      <td>8.726265</td>\n",
       "      <td>[-6.1750116, -6.28487, -6.047803, -6.19385, -5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>20</td>\n",
       "      <td>5444</td>\n",
       "      <td>voted</td>\n",
       "      <td>2024</td>\n",
       "      <td>are</td>\n",
       "      <td>7.347785</td>\n",
       "      <td>[-6.666068, -6.673855, -6.6032705, -6.706757, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>38</td>\n",
       "      <td>2156</td>\n",
       "      <td>see</td>\n",
       "      <td>2003</td>\n",
       "      <td>is</td>\n",
       "      <td>2.909826</td>\n",
       "      <td>[-7.3598285, -7.4892025, -7.4325323, -7.161677...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>40</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "      <td>the</td>\n",
       "      <td>1.209955</td>\n",
       "      <td>[-7.239806, -7.266509, -7.0919533, -7.151936, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>Ugh ffs, most indies don't vote. I don't know ...</td>\n",
       "      <td>[[CLS], u, ##gh, ff, ##s, ,, most, indies, don...</td>\n",
       "      <td>53</td>\n",
       "      <td>1024</td>\n",
       "      <td>:</td>\n",
       "      <td>1012</td>\n",
       "      <td>.</td>\n",
       "      <td>4.274540</td>\n",
       "      <td>[-8.629343, -8.646674, -8.653973, -8.456076, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>I will never denounce Mulldog. [URL]</td>\n",
       "      <td>[[CLS], i, will, never, den, ##oun, ##ce, mu, ...</td>\n",
       "      <td>12</td>\n",
       "      <td>24471</td>\n",
       "      <td>ur</td>\n",
       "      <td>1038</td>\n",
       "      <td>b</td>\n",
       "      <td>3.629005</td>\n",
       "      <td>[-5.546036, -5.6705723, -6.0880394, -5.5258126...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>hey obama fans you all got cucked, once you co...</td>\n",
       "      <td>[[CLS], hey, obama, fans, you, all, got, cu, #...</td>\n",
       "      <td>10</td>\n",
       "      <td>2320</td>\n",
       "      <td>once</td>\n",
       "      <td>2065</td>\n",
       "      <td>if</td>\n",
       "      <td>5.177674</td>\n",
       "      <td>[-7.9113235, -7.7681007, -7.589546, -7.6268983...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>hey obama fans you all got cucked, once you co...</td>\n",
       "      <td>[[CLS], hey, obama, fans, you, all, got, cu, #...</td>\n",
       "      <td>14</td>\n",
       "      <td>3408</td>\n",
       "      <td>terms</td>\n",
       "      <td>3408</td>\n",
       "      <td>terms</td>\n",
       "      <td>0.145805</td>\n",
       "      <td>[-7.741465, -8.198416, -7.470524, -7.724697, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    case_id                                               text  \\\n",
       "0         0  Before that I was down to just go and vote for...   \n",
       "1         0  Before that I was down to just go and vote for...   \n",
       "2         0  Before that I was down to just go and vote for...   \n",
       "3         0  Before that I was down to just go and vote for...   \n",
       "4         0  Before that I was down to just go and vote for...   \n",
       "5         1                                He was a character.   \n",
       "6         1                                He was a character.   \n",
       "7         1                                He was a character.   \n",
       "8         2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "9         2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "10        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "11        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "12        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "13        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "14        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "15        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "16        2  Ugh ffs, most indies don't vote. I don't know ...   \n",
       "17        3               I will never denounce Mulldog. [URL]   \n",
       "18        4  hey obama fans you all got cucked, once you co...   \n",
       "19        4  hey obama fans you all got cucked, once you co...   \n",
       "\n",
       "                                       tokenized_text  masked_token_array_id  \\\n",
       "0   [[CLS], before, that, i, was, down, to, just, ...                     14   \n",
       "1   [[CLS], before, that, i, was, down, to, just, ...                     15   \n",
       "2   [[CLS], before, that, i, was, down, to, just, ...                     17   \n",
       "3   [[CLS], before, that, i, was, down, to, just, ...                     26   \n",
       "4   [[CLS], before, that, i, was, down, to, just, ...                     39   \n",
       "5            [[CLS], he, was, a, character, ., [SEP]]                      1   \n",
       "6            [[CLS], he, was, a, character, ., [SEP]]                      4   \n",
       "7            [[CLS], he, was, a, character, ., [SEP]]                      5   \n",
       "8   [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                      6   \n",
       "9   [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     14   \n",
       "10  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     16   \n",
       "11  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     18   \n",
       "12  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     19   \n",
       "13  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     20   \n",
       "14  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     38   \n",
       "15  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     40   \n",
       "16  [[CLS], u, ##gh, ff, ##s, ,, most, indies, don...                     53   \n",
       "17  [[CLS], i, will, never, den, ##oun, ##ce, mu, ...                     12   \n",
       "18  [[CLS], hey, obama, fans, you, all, got, cu, #...                     10   \n",
       "19  [[CLS], hey, obama, fans, you, all, got, cu, #...                     14   \n",
       "\n",
       "    masked_token_vocab_id masked_token_text  top_pred_token_vocab_id  \\\n",
       "0                    2000                to                     2521   \n",
       "1                    2562              keep                     2185   \n",
       "2                   20687             renew                    20410   \n",
       "3                    1998               and                     1998   \n",
       "4                    1996               the                     1996   \n",
       "5                    2002                he                     2002   \n",
       "6                    2839         character                     2839   \n",
       "7                    1012                 .                     1012   \n",
       "8                    2087              most                     2225   \n",
       "9                    2123               don                     2123   \n",
       "10                   1056                 t                     1056   \n",
       "11                   2129               how                    13898   \n",
       "12                   2116              many                     2001   \n",
       "13                   5444             voted                     2024   \n",
       "14                   2156               see                     2003   \n",
       "15                   1996               the                     1996   \n",
       "16                   1024                 :                     1012   \n",
       "17                  24471                ur                     1038   \n",
       "18                   2320              once                     2065   \n",
       "19                   3408             terms                     3408   \n",
       "\n",
       "   top_pred_token_text   ce_loss  \\\n",
       "0                  far  6.496281   \n",
       "1                 away  6.685265   \n",
       "2               verify  2.409274   \n",
       "3                  and  0.000039   \n",
       "4                  the  0.123853   \n",
       "5                   he  1.577117   \n",
       "6            character  0.032695   \n",
       "7                    .  0.203213   \n",
       "8                 west  6.531352   \n",
       "9                  don  0.061240   \n",
       "10                   t  0.000083   \n",
       "11              rodney  7.105103   \n",
       "12                 was  8.726265   \n",
       "13                 are  7.347785   \n",
       "14                  is  2.909826   \n",
       "15                 the  1.209955   \n",
       "16                   .  4.274540   \n",
       "17                   b  3.629005   \n",
       "18                  if  5.177674   \n",
       "19               terms  0.145805   \n",
       "\n",
       "                                          pred_logits  \n",
       "0   [-7.53187, -7.481711, -7.5258093, -7.5237474, ...  \n",
       "1   [-7.332101, -7.394904, -7.4043255, -7.4488153,...  \n",
       "2   [-4.0348964, -4.034002, -4.3296347, -4.0737686...  \n",
       "3   [-14.3275175, -14.694819, -14.702918, -14.3263...  \n",
       "4   [-9.652537, -9.60094, -9.601458, -9.620717, -9...  \n",
       "5   [-5.992837, -6.0397696, -5.9919176, -5.994485,...  \n",
       "6   [-9.127079, -9.143774, -9.267379, -9.157009, -...  \n",
       "7   [-7.7447324, -7.534629, -7.872818, -7.649254, ...  \n",
       "8   [-8.067986, -7.9866085, -7.9007835, -7.8676376...  \n",
       "9   [-6.1624947, -5.961982, -6.1122904, -6.1582136...  \n",
       "10  [-14.047754, -14.1933775, -14.06629, -13.89654...  \n",
       "11  [-7.405884, -7.4077444, -7.314245, -7.3268056,...  \n",
       "12  [-6.1750116, -6.28487, -6.047803, -6.19385, -5...  \n",
       "13  [-6.666068, -6.673855, -6.6032705, -6.706757, ...  \n",
       "14  [-7.3598285, -7.4892025, -7.4325323, -7.161677...  \n",
       "15  [-7.239806, -7.266509, -7.0919533, -7.151936, ...  \n",
       "16  [-8.629343, -8.646674, -8.653973, -8.456076, -...  \n",
       "17  [-5.546036, -5.6705723, -6.0880394, -5.5258126...  \n",
       "18  [-7.9113235, -7.7681007, -7.589546, -7.6268983...  \n",
       "19  [-7.741465, -8.198416, -7.470524, -7.724697, -...  "
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
