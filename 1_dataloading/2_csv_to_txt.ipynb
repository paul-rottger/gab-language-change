{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "solar-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import fasttext\n",
    "import emoji\n",
    "\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-dimension",
   "metadata": {},
   "source": [
    "## Load Unlabelled Gab Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cathedral-honey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "CPU times: user 2min 58s, sys: 5.28 s, total: 3min 4s\n",
      "Wall time: 3min 8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello world!</td>\n",
       "      <td>2016-08-10 06:58:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@a or @e: do you have a stays page with number...</td>\n",
       "      <td>2016-08-21 22:58:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#100followers is this in reference to Lord Kek...</td>\n",
       "      <td>2016-08-24 03:19:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#BoycottTarget is costing more than anyone exp...</td>\n",
       "      <td>2016-08-25 09:58:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>All those white tears mugs are filled with thi...</td>\n",
       "      <td>2016-08-26 18:53:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>Antwort: TERROR!</td>\n",
       "      <td>2018-10-28 15:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>Fraude fake - Cuidado! 17 voto nulo. Bolsonaro...</td>\n",
       "      <td>2018-10-28 18:24:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3421</th>\n",
       "      <td>Wrong as usual little Kiwi. You monkeys lose a...</td>\n",
       "      <td>2018-10-28 21:11:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>https://www.youtube.com/watch?v=SGWizajL7tA</td>\n",
       "      <td>2018-10-28 23:39:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>Not at all. Perhaps you haven't looked far eno...</td>\n",
       "      <td>2018-10-29 02:13:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3424 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text          created_at\n",
       "0                                          Hello world! 2016-08-10 06:58:37\n",
       "1     @a or @e: do you have a stays page with number... 2016-08-21 22:58:34\n",
       "2     #100followers is this in reference to Lord Kek... 2016-08-24 03:19:39\n",
       "3     #BoycottTarget is costing more than anyone exp... 2016-08-25 09:58:27\n",
       "4     All those white tears mugs are filled with thi... 2016-08-26 18:53:31\n",
       "...                                                 ...                 ...\n",
       "3419                                   Antwort: TERROR! 2018-10-28 15:36:00\n",
       "3420  Fraude fake - Cuidado! 17 voto nulo. Bolsonaro... 2018-10-28 18:24:57\n",
       "3421  Wrong as usual little Kiwi. You monkeys lose a... 2018-10-28 21:11:21\n",
       "3422        https://www.youtube.com/watch?v=SGWizajL7tA 2018-10-28 23:39:25\n",
       "3423  Not at all. Perhaps you haven't looked far eno... 2018-10-29 02:13:03\n",
       "\n",
       "[3424 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load texts from unlabelled corpus into set\n",
    "\n",
    "# initialise empty lists --> faster than appending to dict\n",
    "texts = []\n",
    "dates = []\n",
    "\n",
    "# initialise counter var for counting iterations\n",
    "counter = 0\n",
    "\n",
    "sample_freq = 10000 # sample every n-th post with n = sample_freq\n",
    "print_freq = 1000000 # print progress every n posts with n = print_freq\n",
    "max_counter = 1000000\n",
    "\n",
    "# iterate over each line\n",
    "with open('../0_data/gabposts_clean_170221.csv', 'r') as read_obj:\n",
    "    csv_dict_reader = csv.DictReader(x.replace('\\0', '') for x in read_obj)\n",
    "    for row in csv_dict_reader:\n",
    "        if counter % sample_freq == 0:\n",
    "            texts.append(row['text'])\n",
    "            dates.append(row['created_at'])\n",
    "        counter+=1\n",
    "        if counter % print_freq == 0:\n",
    "            print(counter)\n",
    "\n",
    "# create dataframe from lists\n",
    "texts = pd.Series(texts, name = 'text')\n",
    "dates = pd.Series(dates, name = 'created_at')\n",
    "sample_df = pd.concat([texts, dates], axis=1)\n",
    "\n",
    "# clear out RAM\n",
    "del texts\n",
    "del dates\n",
    "\n",
    "# convert dtypes\n",
    "sample_df['created_at']= sample_df.created_at.astype('datetime64')\n",
    "sample_df['text']= sample_df.text.astype('string')\n",
    "\n",
    "# print finished df\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-trade",
   "metadata": {},
   "source": [
    "## Perform Additional Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "herbal-medicine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281 posts, of which 0 were dropped for empty string content\n",
      "3281 posts remain. \n",
      "\n",
      "CPU times: user 77.1 ms, sys: 2.15 ms, total: 79.3 ms\n",
      "Wall time: 78.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define function to clean text\n",
    "def clean(text):\n",
    "\n",
    "    # convert html\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # replace mentions, URLs and emojis with special token\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    text = ''.join('[EMOJI]' if (char in emoji.UNICODE_EMOJI['en']) else char for char in text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# create clean_text column\n",
    "sample_df['clean_text'] = sample_df.text.apply(clean)\n",
    "\n",
    "\n",
    "# save number of documents before dropping empty posts\n",
    "n_docs = sample_df.shape[0]\n",
    "\n",
    "# drop rows with empty text\n",
    "sample_df = sample_df[sample_df.clean_text!=\"\"]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - sample_df.shape[0]} were dropped for empty string content')\n",
    "print(f'{sample_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "stuffed-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of sample_df to avoid having to reload sample_df\n",
    "text_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "scenic-organization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281 posts, of which 268 were dropped for (most likely) not being in English.\n",
      "3013 posts remain. \n",
      "\n",
      "CPU times: user 319 ms, sys: 199 ms, total: 518 ms\n",
      "Wall time: 632 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# check language\n",
    "fmodel = fasttext.load_model('../0_models/lang_detect/lid.176.bin')\n",
    "\n",
    "def check_language(text):\n",
    "    predictions = fmodel.predict(text, k=3)\n",
    "    \n",
    "    # if top prediction is certain and not English, return non-English\n",
    "    if (predictions[0][0]!='__label__en') and (predictions[1][0]>0.50):\n",
    "        return 'non-English'\n",
    "    \n",
    "    # else if English is one of top 3 predictions, return English\n",
    "    elif '__label__en' in predictions[0]:\n",
    "        return 'English'\n",
    "    \n",
    "    # else return non-English\n",
    "    else:\n",
    "        return 'non-English'\n",
    "\n",
    "# save number of documents before dropping non-English posts\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop non-English posts\n",
    "text_df = text_df[text_df.text.apply(lambda x: check_language(x) == 'English')]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for (most likely) not being in English.')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-occasions",
   "metadata": {},
   "source": [
    "## Write to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "double-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 100\n",
    "TEST_SIZE = 100\n",
    "\n",
    "def write_to_txt(text_series, train_size, test_size):\n",
    "    export_train, export_eval = train_test_split(text_series, train_size = train_size, test_size = test_size)\n",
    "\n",
    "    with open('../0_data/clean/train.txt', 'w') as write_obj:\n",
    "        for text in export_train:\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "\n",
    "    with open('../0_data/clean/eval.txt', 'w') as write_obj:\n",
    "        for text in export_eval:\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "\n",
    "write_to_txt(text_df.clean_text, TRAIN_SIZE, TEST_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
