{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "earlier-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import fasttext\n",
    "import emoji\n",
    "\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-frontier",
   "metadata": {},
   "source": [
    "## Load Unlabelled Gab Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hundred-metropolitan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "CPU times: user 3min 1s, sys: 7.42 s, total: 3min 8s\n",
      "Wall time: 3min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello world!</td>\n",
       "      <td>2016-08-10 06:58:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcome to Gab, @Vince</td>\n",
       "      <td>2016-08-10 17:23:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Testing a link: http://regated.com/2016/08/fat...</td>\n",
       "      <td>2016-08-10 22:58:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just setting up my gabr.</td>\n",
       "      <td>2016-08-11 04:16:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Death to Islam!</td>\n",
       "      <td>2016-08-11 17:20:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423321</th>\n",
       "      <td>Hmmm. That does indeed seem super sketchy. No ...</td>\n",
       "      <td>2018-10-29 03:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423322</th>\n",
       "      <td>So all 3 were in line for the Rabbi? Did they ...</td>\n",
       "      <td>2018-10-29 03:00:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423323</th>\n",
       "      <td>ela apoiou o Doria e o Major Olimpo foi com o ...</td>\n",
       "      <td>2018-10-29 03:00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423324</th>\n",
       "      <td>Military people understand gun safety and the ...</td>\n",
       "      <td>2018-10-29 03:00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423325</th>\n",
       "      <td>https://www.youtube.com/watch?v=bMK0MIwWzHI Re...</td>\n",
       "      <td>2018-10-29 03:00:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3423326 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text          created_at\n",
       "0                                             Hello world! 2016-08-10 06:58:37\n",
       "1                                   Welcome to Gab, @Vince 2016-08-10 17:23:03\n",
       "2        Testing a link: http://regated.com/2016/08/fat... 2016-08-10 22:58:35\n",
       "3                                 just setting up my gabr. 2016-08-11 04:16:16\n",
       "4                                          Death to Islam! 2016-08-11 17:20:52\n",
       "...                                                    ...                 ...\n",
       "3423321  Hmmm. That does indeed seem super sketchy. No ... 2018-10-29 03:00:02\n",
       "3423322  So all 3 were in line for the Rabbi? Did they ... 2018-10-29 03:00:15\n",
       "3423323  ela apoiou o Doria e o Major Olimpo foi com o ... 2018-10-29 03:00:21\n",
       "3423324  Military people understand gun safety and the ... 2018-10-29 03:00:35\n",
       "3423325  https://www.youtube.com/watch?v=bMK0MIwWzHI Re... 2018-10-29 03:00:45\n",
       "\n",
       "[3423326 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load texts from unlabelled corpus into set\n",
    "\n",
    "# initialise empty lists --> faster than appending to dict\n",
    "texts = []\n",
    "dates = []\n",
    "\n",
    "# initialise counter var for counting iterations\n",
    "counter = 0\n",
    "\n",
    "sample_freq = 10 # sample every n-th post with n = sample_freq\n",
    "print_freq = 1000000 # print progress every n posts with n = print_freq\n",
    "\n",
    "# iterate over each line\n",
    "with open('../0_data/raw/gabposts_clean_170221.csv', 'r') as read_obj:\n",
    "    csv_dict_reader = csv.DictReader(x.replace('\\0', '') for x in read_obj)\n",
    "    for row in csv_dict_reader:\n",
    "        if counter % sample_freq == 0:\n",
    "            texts.append(row['text'])\n",
    "            dates.append(row['created_at'])\n",
    "        counter+=1\n",
    "        if counter % print_freq == 0:\n",
    "            print(counter)\n",
    "\n",
    "# create dataframe from lists\n",
    "texts = pd.Series(texts, name = 'text')\n",
    "dates = pd.Series(dates, name = 'created_at')\n",
    "sample_df = pd.concat([texts, dates], axis=1)\n",
    "\n",
    "# clear out RAM\n",
    "del texts\n",
    "del dates\n",
    "\n",
    "# convert dtypes\n",
    "sample_df['created_at']= sample_df.created_at.astype('datetime64')\n",
    "sample_df['text']= sample_df.text.astype('string')\n",
    "\n",
    "# print finished df\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-thompson",
   "metadata": {},
   "source": [
    "## Perform Additional Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "excess-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of sample_df to avoid having to reload sample_df\n",
    "text_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "convinced-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3423326 posts, of which 139269 were dropped for empty string content\n",
      "3284057 posts remain. \n",
      "\n",
      "3284057 posts, of which 284647 were dropped for empty string content\n",
      "2999410 posts remain. \n",
      "\n",
      "CPU times: user 1min 11s, sys: 1.24 s, total: 1min 12s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define function to clean text\n",
    "def clean(text):\n",
    "\n",
    "    # convert html\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # replace mentions, URLs and emojis with special token\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    text = ''.join('[EMOJI]' if (char in emoji.UNICODE_EMOJI['en']) else char for char in text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# create clean_text column\n",
    "text_df['clean_text'] = text_df.text.apply(clean)\n",
    "\n",
    "\n",
    "# save number of documents before dropping empty posts\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop rows with empty text\n",
    "text_df = text_df[text_df.clean_text!=\"\"]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for empty string content')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')\n",
    "\n",
    "\n",
    "# save number of documents before dropping posts that are just [URL], [EMOJI] or [USER]\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop rows with text that is just [URL], [EMOJI] or [USER]\n",
    "text_df = text_df[text_df.clean_text!=\"[URL]\"]\n",
    "text_df = text_df[text_df.clean_text!=\"[EMOJI]\"]\n",
    "text_df = text_df[text_df.clean_text!=\"[USER]\"]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for being just [URL], [EMOJI] or [USER]')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "jewish-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2999410 posts, of which 205802 were dropped for (most likely) not being in English.\n",
      "2793608 posts remain. \n",
      "\n",
      "CPU times: user 2min 3s, sys: 2 s, total: 2min 5s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# check language\n",
    "fmodel = fasttext.load_model('../0_models/lang_detect/lid.176.bin')\n",
    "\n",
    "def check_language(text):\n",
    "    predictions = fmodel.predict(text, k=3)\n",
    "    \n",
    "    # if top prediction is certain and not English, return non-English\n",
    "    if (predictions[0][0]!='__label__en') and (predictions[1][0]>0.50):\n",
    "        return 'non-English'\n",
    "    \n",
    "    # else if English is one of top 3 predictions, return English\n",
    "    elif '__label__en' in predictions[0]:\n",
    "        return 'English'\n",
    "    \n",
    "    # else return non-English\n",
    "    else:\n",
    "        return 'non-English'\n",
    "\n",
    "# save number of documents before dropping non-English posts\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop non-English posts\n",
    "text_df = text_df[text_df.text.apply(lambda x: check_language(x) == 'English')]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for (most likely) not being in English.')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-quality",
   "metadata": {},
   "source": [
    "## Write to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "computational-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 1000000\n",
    "TEST_SIZE = 10000\n",
    "\n",
    "def write_to_txt(text_series, train_size, test_size):\n",
    "    export_train, export_eval = train_test_split(text_series, train_size = train_size, test_size = test_size)\n",
    "\n",
    "    with open('../0_data/clean/train.txt', 'w') as write_obj:\n",
    "        for text in export_train:\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "\n",
    "    with open('../0_data/clean/eval.txt', 'w') as write_obj:\n",
    "        for text in export_eval:\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "\n",
    "write_to_txt(text_df.clean_text, TRAIN_SIZE, TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-fifth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
