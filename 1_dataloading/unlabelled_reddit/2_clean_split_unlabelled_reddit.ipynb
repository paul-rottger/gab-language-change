{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "western-smart",
   "metadata": {},
   "source": [
    "# Clean and Split Monthly Unlabelled Reddit (News) Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noble-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import csv\n",
    "import re\n",
    "import fasttext\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import os\n",
    "\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-hawaiian",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stable-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_df(filepath):\n",
    "\n",
    "    with open(os.path.join(directory, filename),'r', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        cols = [next(reader)]\n",
    "        df = pd.DataFrame(line for line in reader)\n",
    "        df.columns = cols\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "        \n",
    "    # small number of csv formatting errors --> delete\n",
    "\n",
    "    # save number of documents\n",
    "    n_docs = df.shape[0]\n",
    "\n",
    "    # drop posts with formatting errors\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    print(f'{n_docs} posts, of which {n_docs - df.shape[0]} were dropped for csv formatting errors.')\n",
    "    print(f'{df.shape[0]} posts remain. \\n')\n",
    "    \n",
    "    # convert epoch to readable datetime\n",
    "    df.created_utc = df.created_utc.apply(lambda x: dt.datetime.fromtimestamp(int(x)))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boxed-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, max_length = 1024):\n",
    "\n",
    "    # convert html\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # clean unicode formatting errors\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.replace('\\u200d', '')\n",
    "    \n",
    "    # truncate text to max_length\n",
    "    text = text[:max_length]\n",
    "    \n",
    "    # remove newline and tab characters\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    \n",
    "    # replace URLs and emojis with special tokens\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    text = ''.join('[EMOJI]' if (char in emoji.UNICODE_EMOJI['en']) else char for char in text).strip()\n",
    "    \n",
    "    # remove deleted posts\n",
    "    text = text.replace('[deleted]','')\n",
    "    text = text.replace('[removed]','')\n",
    "    \n",
    "    # remove leading \">\" (reddit artifact)\n",
    "    text = text.lstrip('>')\n",
    "    \n",
    "    # collapse whitespace into single whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # truncate text to max_length again\n",
    "    text = text[:max_length]\n",
    "    \n",
    "    # remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identified-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty(df):\n",
    "    \n",
    "    # save number of documents before dropping empty posts\n",
    "    n_docs = df.shape[0]\n",
    "\n",
    "    # drop rows with empty text\n",
    "    df = df[df.clean_text.values!=\"\"]\n",
    "\n",
    "    print(f'{n_docs} posts, of which {n_docs - df.shape[0]} were dropped for empty string content')\n",
    "    print(f'{df.shape[0]} posts remain. \\n')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "returning-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_url_emoji(df):\n",
    "\n",
    "    # save number of documents before dropping posts that are just [URL] or [EMOJI]\n",
    "    n_docs = df.shape[0]\n",
    "\n",
    "    # drop rows with text that is just [URL] or [EMOJI]\n",
    "    df = df[(df.clean_text!=\"[URL]\") & (df.clean_text!=\"[EMOJI]\")]\n",
    "\n",
    "    print(f'{n_docs} posts, of which {n_docs - df.shape[0]} were dropped for being just [URL] or [EMOJI]')\n",
    "    print(f'{df.shape[0]} posts remain. \\n')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chicken-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_english(df):\n",
    "    \n",
    "    # load language classifier\n",
    "    fmodel = fasttext.load_model('../../0_models/lang_detect/lid.176.bin')\n",
    "\n",
    "    def check_language(text):\n",
    "        predictions = fmodel.predict(text, k=3)\n",
    "\n",
    "        # if top prediction is certain and not English, return non-English\n",
    "        if (predictions[0][0]!='__label__en') and (predictions[1][0]>0.50):\n",
    "            return 'non-English'\n",
    "\n",
    "        # else if English is one of top 3 predictions, return English\n",
    "        elif '__label__en' in predictions[0]:\n",
    "            return 'English'\n",
    "\n",
    "        # else return non-English\n",
    "        else:\n",
    "            return 'non-English'\n",
    "\n",
    "    # save number of documents before dropping non-English posts\n",
    "    n_docs = df.shape[0]\n",
    "\n",
    "    # drop non-English posts\n",
    "    df = df[df.clean_text.apply(lambda x: check_language(x) == 'English')]\n",
    "\n",
    "    print(f'{n_docs} posts, of which {n_docs - df.shape[0]} were dropped for (most likely) not being in English.')\n",
    "    print(f'{df.shape[0]} posts remain. \\n')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "welsh-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dupl(df):\n",
    "\n",
    "    # save number of documents before dropping duplicates\n",
    "    n_docs = df.shape[0]\n",
    "\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(subset = ['clean_text'], inplace=True)\n",
    "\n",
    "    print(f'{n_docs} posts, of which {n_docs - df.shape[0]} were dropped for being duplicates.')\n",
    "    print(f'{df.shape[0]} posts remain. \\n')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "right-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test splits for each month-year\n",
    "    \n",
    "def split_export(df, filename):\n",
    "    \n",
    "    export_train = pd.Series(dtype=str)\n",
    "    export_test = export_train.copy()\n",
    "    \n",
    "    for subreddit in pd.unique(df.subreddit):\n",
    "        \n",
    "        add_train, add_test = train_test_split(df[df.subreddit==subreddit].clean_text,\n",
    "                                                     train_size = 500000, test_size = 5000,\n",
    "                                                     random_state = 123)\n",
    "        \n",
    "        export_train = export_train.append(add_train)\n",
    "        export_test = export_test.append(add_test)\n",
    "        \n",
    "    \n",
    "    with open(f'../../0_data/clean/unlabelled_reddit/train_{filename[5:-4]}_1m.txt', 'w') as write_obj:\n",
    "        for text in export_train.sample(frac=1):\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "            \n",
    "    with open(f'../../0_data/clean/unlabelled_reddit/test_{filename[5:-4]}_10k.txt', 'w') as write_obj:\n",
    "        for text in export_test.sample(frac=1):\n",
    "            write_obj.write(text + \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "seven-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test splits for each month-year\n",
    "# for months where there are more than 1m comments in total but less than 500k in one of the two subreddits\n",
    "# the downstream effects of this slight imbalance should be negligible\n",
    "    \n",
    "def split_export_unbalanced(df, filename):\n",
    "    \n",
    "    export_train, export_test = train_test_split(df.clean_text,\n",
    "                                                 train_size = 1000000, test_size = 10000,\n",
    "                                                 random_state = 123)\n",
    "    \n",
    "    with open(f'../../0_data/clean/unlabelled_reddit/train_{filename[5:-4]}_1m.txt', 'w') as write_obj:\n",
    "        for text in export_train.sample(frac=1):\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "            \n",
    "    with open(f'../../0_data/clean/unlabelled_reddit/test_{filename[5:-4]}_10k.txt', 'w') as write_obj:\n",
    "        for text in export_test.sample(frac=1):\n",
    "            write_obj.write(text + \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-slave",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "trying-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_split(directory, filename):\n",
    "    \n",
    "    filepath = os.path.join(directory, filename)\n",
    "    \n",
    "    # load df from csv\n",
    "    df = load_csv_to_df(filepath)\n",
    "    \n",
    "    # clean text (\"body\") and write to column\n",
    "    df['clean_text'] = df.body.apply(clean_text)\n",
    "    \n",
    "    # drop posts with empty string \n",
    "    df = drop_empty(df)\n",
    "    \n",
    "    # drop posts that are just [URL] or [EMOJI]\n",
    "    df = drop_url_emoji(df)\n",
    "    \n",
    "    # drop posts that are likely not English\n",
    "    df = drop_non_english(df)\n",
    "    \n",
    "    # drop duplicates\n",
    "    df = drop_dupl(df)\n",
    "    \n",
    "    # export to train and test file\n",
    "    split_export_unbalanced(df, filename)\n",
    "    \n",
    "    return f'wrote cleaned train and test file from {filename}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "identified-bathroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWS_2019_02.CSV \n",
      "\n",
      "1347478 posts, of which 0 were dropped for csv formatting errors.\n",
      "1347478 posts remain. \n",
      "\n",
      "1347478 posts, of which 196135 were dropped for empty string content\n",
      "1151343 posts remain. \n",
      "\n",
      "1151343 posts, of which 2542 were dropped for being just [URL] or [EMOJI]\n",
      "1148801 posts remain. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148801 posts, of which 7534 were dropped for (most likely) not being in English.\n",
      "1141267 posts remain. \n",
      "\n",
      "1141267 posts, of which 24215 were dropped for being duplicates.\n",
      "1117052 posts remain. \n",
      "\n",
      "NEWS_2019_04.CSV \n",
      "\n",
      "1487245 posts, of which 0 were dropped for csv formatting errors.\n",
      "1487245 posts remain. \n",
      "\n",
      "1487245 posts, of which 221194 were dropped for empty string content\n",
      "1266051 posts remain. \n",
      "\n",
      "1266051 posts, of which 2701 were dropped for being just [URL] or [EMOJI]\n",
      "1263350 posts remain. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1263350 posts, of which 8042 were dropped for (most likely) not being in English.\n",
      "1255308 posts remain. \n",
      "\n",
      "1255308 posts, of which 26513 were dropped for being duplicates.\n",
      "1228795 posts remain. \n",
      "\n",
      "NEWS_2019_05.CSV \n",
      "\n",
      "1380123 posts, of which 0 were dropped for csv formatting errors.\n",
      "1380123 posts remain. \n",
      "\n",
      "1380123 posts, of which 194131 were dropped for empty string content\n",
      "1185992 posts remain. \n",
      "\n",
      "1185992 posts, of which 2415 were dropped for being just [URL] or [EMOJI]\n",
      "1183577 posts remain. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183577 posts, of which 7328 were dropped for (most likely) not being in English.\n",
      "1176249 posts remain. \n",
      "\n",
      "1176249 posts, of which 27958 were dropped for being duplicates.\n",
      "1148291 posts remain. \n",
      "\n",
      "NEWS_2019_06.CSV \n",
      "\n",
      "1469294 posts, of which 2 were dropped for csv formatting errors.\n",
      "1469292 posts remain. \n",
      "\n",
      "1469292 posts, of which 224535 were dropped for empty string content\n",
      "1244757 posts remain. \n",
      "\n",
      "1244757 posts, of which 2716 were dropped for being just [URL] or [EMOJI]\n",
      "1242041 posts remain. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242041 posts, of which 7961 were dropped for (most likely) not being in English.\n",
      "1234080 posts remain. \n",
      "\n",
      "1234080 posts, of which 23497 were dropped for being duplicates.\n",
      "1210583 posts remain. \n",
      "\n",
      "NEWS_2019_11.CSV \n",
      "\n",
      "1447417 posts, of which 0 were dropped for csv formatting errors.\n",
      "1447417 posts remain. \n",
      "\n",
      "1447417 posts, of which 220980 were dropped for empty string content\n",
      "1226437 posts remain. \n",
      "\n",
      "1226437 posts, of which 2616 were dropped for being just [URL] or [EMOJI]\n",
      "1223821 posts remain. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223821 posts, of which 8201 were dropped for (most likely) not being in English.\n",
      "1215620 posts remain. \n",
      "\n",
      "1215620 posts, of which 24729 were dropped for being duplicates.\n",
      "1190891 posts remain. \n",
      "\n",
      "NEWS_2019_12.CSV \n",
      "\n",
      "1613646 posts, of which 3 were dropped for csv formatting errors.\n",
      "1613643 posts remain. \n",
      "\n",
      "1613643 posts, of which 242849 were dropped for empty string content\n",
      "1370794 posts remain. \n",
      "\n",
      "1370794 posts, of which 2554 were dropped for being just [URL] or [EMOJI]\n",
      "1368240 posts remain. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368240 posts, of which 8485 were dropped for (most likely) not being in English.\n",
      "1359755 posts remain. \n",
      "\n",
      "1359755 posts, of which 27331 were dropped for being duplicates.\n",
      "1332424 posts remain. \n",
      "\n",
      "CPU times: user 13min 32s, sys: 26.7 s, total: 13min 59s\n",
      "Wall time: 14min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load raw data from csvs, clean it and split it into train and test sets\n",
    "\n",
    "directory = '../../0_data/raw/unlabelled_reddit'\n",
    "\n",
    "for filename in [\"news_2019_02.csv\", \"news_2019_04.csv\", \"news_2019_05.csv\", \"news_2019_06.csv\", \"news_2019_11.csv\", \"news_2019_12.csv\"]: #os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        print(filename.upper(), '\\n')\n",
    "        try:\n",
    "            clean_split(directory, filename)\n",
    "        except:\n",
    "            print('not enough data')\n",
    "            continue\n",
    "        continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-plant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
