{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlimited-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import fasttext\n",
    "import emoji\n",
    "import unicodedata\n",
    "\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-float",
   "metadata": {},
   "source": [
    "## Load Unlabelled Gab Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gothic-airplane",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_texts' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load texts from unlabelled corpus into set\n",
    "\n",
    "# initialise empty lists --> faster than appending to dict\n",
    "texts = []\n",
    "dates = []\n",
    "\n",
    "# initialise counter var for counting iterations\n",
    "counter = 0\n",
    "\n",
    "sample_freq = 2 # sample every n-th post with n = sample_freq\n",
    "print_freq = 1000000 # print progress every n posts with n = print_freq\n",
    "\n",
    "# iterate over each line\n",
    "with open('../../0_data/raw/gabposts_clean_170221.csv', 'r') as read_obj:\n",
    "    csv_dict_reader = csv.DictReader(x.replace('\\0', '') for x in read_obj)\n",
    "    for row in csv_dict_reader:\n",
    "        if counter % sample_freq == 0:\n",
    "            texts.append(row['text'])\n",
    "            dates.append(row['created_at'])\n",
    "        counter+=1\n",
    "        if counter % print_freq == 0:\n",
    "            print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hispanic-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello world!</td>\n",
       "      <td>2016-08-10 06:58:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test post for repost</td>\n",
       "      <td>2016-08-10 07:26:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All censorships exist to prevent anyone from c...</td>\n",
       "      <td>2016-08-10 07:59:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H E L L O</td>\n",
       "      <td>2016-08-10 11:03:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gab is to speak as ________ is to ________ #fr...</td>\n",
       "      <td>2016-08-10 11:44:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116624</th>\n",
       "      <td>I guess you will have to go to jail. If that i...</td>\n",
       "      <td>2018-10-29 03:00:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116625</th>\n",
       "      <td>https://www.youtube.com/watch?v=bMK0MIwWzHI Re...</td>\n",
       "      <td>2018-10-29 03:00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116626</th>\n",
       "      <td>Thank you, been trolling these cunts all day.</td>\n",
       "      <td>2018-10-29 03:01:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116627</th>\n",
       "      <td>Very true.. #BigPharma #corruption</td>\n",
       "      <td>2018-10-29 03:02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17116628</th>\n",
       "      <td>https://thedailycoin.org/2018/10/28/movie-prop...</td>\n",
       "      <td>2018-10-29 03:05:45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17116629 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text  \\\n",
       "0                                              Hello world!   \n",
       "1                                      test post for repost   \n",
       "2         All censorships exist to prevent anyone from c...   \n",
       "3                                                 H E L L O   \n",
       "4         Gab is to speak as ________ is to ________ #fr...   \n",
       "...                                                     ...   \n",
       "17116624  I guess you will have to go to jail. If that i...   \n",
       "17116625  https://www.youtube.com/watch?v=bMK0MIwWzHI Re...   \n",
       "17116626      Thank you, been trolling these cunts all day.   \n",
       "17116627                 Very true.. #BigPharma #corruption   \n",
       "17116628  https://thedailycoin.org/2018/10/28/movie-prop...   \n",
       "\n",
       "                  created_at  \n",
       "0        2016-08-10 06:58:37  \n",
       "1        2016-08-10 07:26:18  \n",
       "2        2016-08-10 07:59:06  \n",
       "3        2016-08-10 11:03:39  \n",
       "4        2016-08-10 11:44:54  \n",
       "...                      ...  \n",
       "17116624 2018-10-29 03:00:43  \n",
       "17116625 2018-10-29 03:00:45  \n",
       "17116626 2018-10-29 03:01:45  \n",
       "17116627 2018-10-29 03:02:17  \n",
       "17116628 2018-10-29 03:05:45  \n",
       "\n",
       "[17116629 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dataframe from lists\n",
    "texts = pd.Series(texts, name = 'text')\n",
    "dates = pd.Series(dates, name = 'created_at')\n",
    "sample_df = pd.concat([texts, dates], axis=1)\n",
    "\n",
    "# clear out RAM\n",
    "del texts\n",
    "del dates\n",
    "\n",
    "# convert dtypes\n",
    "sample_df['created_at']= sample_df.created_at.astype('datetime64')\n",
    "sample_df['text']= sample_df.text.astype('string')\n",
    "\n",
    "# print finished df\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-flood",
   "metadata": {},
   "source": [
    "## Perform Additional Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oriented-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of sample_df to avoid having to reload sample_df\n",
    "text_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "atlantic-physiology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 18s, sys: 1min 8s, total: 7min 27s\n",
      "Wall time: 9min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define function to clean text\n",
    "def clean(text):\n",
    "\n",
    "    # convert html\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # replace mentions, URLs and emojis with special token\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    text = ''.join('[EMOJI]' if (char in emoji.UNICODE_EMOJI['en']) else char for char in text).strip()\n",
    "    \n",
    "    # clean misformatting (e.g. \"\\xa0\")\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# create clean_text column\n",
    "text_df['clean_text'] = text_df.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "paperback-cowboy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17116629 posts, of which 695435 were dropped for empty string content\n",
      "16421194 posts remain. \n",
      "\n",
      "CPU times: user 11.4 s, sys: 1min 13s, total: 1min 24s\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# save number of documents before dropping empty posts\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop rows with empty text\n",
    "text_df = text_df[text_df.clean_text.values!=\"\"]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for empty string content')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "essential-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16421194 posts, of which 1424791 were dropped for being just [URL], [EMOJI] or [USER]\n",
      "14996403 posts remain. \n",
      "\n",
      "CPU times: user 17 s, sys: 1min 23s, total: 1min 40s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# save number of documents before dropping posts that are just [URL], [EMOJI] or [USER]\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop rows with text that is just [URL], [EMOJI] or [USER]\n",
    "text_df = text_df[(text_df.clean_text!=\"[URL]\") & (text_df.clean_text!=\"[EMOJI]\") & (text_df.clean_text!=\"[USER]\")]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for being just [URL], [EMOJI] or [USER]')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "responsible-exhibit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14996403 posts, of which 1028437 were dropped for (most likely) not being in English.\n",
      "13967966 posts remain. \n",
      "\n",
      "CPU times: user 11min 10s, sys: 1min 57s, total: 13min 7s\n",
      "Wall time: 17min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# check language\n",
    "fmodel = fasttext.load_model('../../0_models/lang_detect/lid.176.bin')\n",
    "\n",
    "def check_language(text):\n",
    "    predictions = fmodel.predict(text, k=3)\n",
    "    \n",
    "    # if top prediction is certain and not English, return non-English\n",
    "    if (predictions[0][0]!='__label__en') and (predictions[1][0]>0.50):\n",
    "        return 'non-English'\n",
    "    \n",
    "    # else if English is one of top 3 predictions, return English\n",
    "    elif '__label__en' in predictions[0]:\n",
    "        return 'English'\n",
    "    \n",
    "    # else return non-English\n",
    "    else:\n",
    "        return 'non-English'\n",
    "\n",
    "# save number of documents before dropping non-English posts\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop non-English posts\n",
    "text_df = text_df[text_df.text.apply(lambda x: check_language(x) == 'English')]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for (most likely) not being in English.')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-france",
   "metadata": {},
   "source": [
    "## Write to Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "visible-simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.6 s, sys: 1min 17s, total: 2min 13s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TRAIN_SIZES = [1000000, 2000000, 5000000, 10000000]\n",
    "TEST_SIZE = 50000\n",
    "\n",
    "export_train_base, export_eval = train_test_split(text_df.clean_text, test_size = TEST_SIZE, random_state = 123)\n",
    "\n",
    "with open(f'../../0_data/clean/unlabelled_pushshift/eval_rand_{int(TEST_SIZE/1000)}k.txt', 'w') as write_obj:\n",
    "    for text in export_eval:\n",
    "        write_obj.write(text + \"\\n \\n\")\n",
    "\n",
    "for size in TRAIN_SIZES:\n",
    "    export_train, _ = train_test_split(export_train_base, train_size = size, random_state = 123)\n",
    "\n",
    "    with open(f'../../0_data/clean/unlabelled_pushshift/train_rand_{int(size/1000000)}m.txt', 'w') as write_obj:\n",
    "        for text in export_train:\n",
    "            write_obj.write(text + \"\\n \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-kansas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
