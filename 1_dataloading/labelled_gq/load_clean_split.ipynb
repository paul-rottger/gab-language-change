{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ideal-freedom",
   "metadata": {},
   "source": [
    "# Load, clean and split labelled GQ data (Qian et al. 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "injured-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import emoji\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-research",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sitting-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load texts from labelled corpus into set\n",
    "\n",
    "# initialise empty lists --> faster than appending to dict\n",
    "texts = []\n",
    "hate_ids = []\n",
    "\n",
    "with open('../../0_data/raw/qian_gab_raw.csv', 'r') as read_obj:\n",
    "    csv_dict_reader = csv.DictReader(x.replace('\\n', ' ') for x in read_obj)\n",
    "    for row in csv_dict_reader:\n",
    "        texts.append(row['text'])\n",
    "        hate_ids.append(row['hate_speech_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "united-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 ms, sys: 6.01 ms, total: 16.7 ms\n",
      "Wall time: 37.5 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. i joined gab to remind myself how retarded ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. This is what the left is really scared of. ...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. It makes you an asshole. 2. \tGive it to a  ...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1. So they manage to provide a whole lot of da...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. Hi there, i,m Keith, i hope you are doing w...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>1. Remember this 3 months ago? The intern that...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>1. #Sweden's four major dailies aftonbladet.se...</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>1. Satisfaction and justice 2. \tBEST TRUMP INS...</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11823</th>\n",
       "      <td>1. Twitter BANNED me and reported me to the FB...</td>\n",
       "      <td>[2, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11824</th>\n",
       "      <td>1. How absolutely unbelievably fascinating? A ...</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11825 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text hate_ids\n",
       "0      1. i joined gab to remind myself how retarded ...      [1]\n",
       "1      1. This is what the left is really scared of. ...      [3]\n",
       "2      1. It makes you an asshole. 2. \tGive it to a  ...      [2]\n",
       "3      1. So they manage to provide a whole lot of da...      [2]\n",
       "4      1. Hi there, i,m Keith, i hope you are doing w...      [3]\n",
       "...                                                  ...      ...\n",
       "11820  1. Remember this 3 months ago? The intern that...      [2]\n",
       "11821  1. #Sweden's four major dailies aftonbladet.se...      [2]\n",
       "11822  1. Satisfaction and justice 2. \tBEST TRUMP INS...      [3]\n",
       "11823  1. Twitter BANNED me and reported me to the FB...   [2, 3]\n",
       "11824  1. How absolutely unbelievably fascinating? A ...      [1]\n",
       "\n",
       "[11825 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dataframe from lists\n",
    "texts = pd.Series(texts, name = 'text')\n",
    "hate_ids = pd.Series(hate_ids, name = 'hate_ids')\n",
    "sample_df = pd.concat([texts, hate_ids], axis=1)\n",
    "\n",
    "\n",
    "# convert dtypes\n",
    "sample_df['text']= sample_df.text.astype('string')\n",
    "\n",
    "# print finished df\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informative-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy to work with\n",
    "import_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "welcome-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text column by enumeration\n",
    "series = import_df.text\n",
    "for i in reversed(range(2,30)):\n",
    "    series = series.str.split(f\"{i}. \\t\").explode()\n",
    "\n",
    "# merge text column with initial index\n",
    "import_df = pd.DataFrame(series).merge(import_df.hate_ids, left_index=True, right_index=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legal-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create thread-internal id to match with hate_ids\n",
    "\n",
    "def count_indices(c):\n",
    "    return list(range(1,c+1))\n",
    "\n",
    "ids = import_df.groupby('index').index.count().apply(lambda x: count_indices(x)).explode()\n",
    "ids.name = 'id'\n",
    "\n",
    "import_df = pd.concat([import_df, pd.DataFrame(ids).reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "grand-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert hate_ids to proper lists\n",
    "import_df['hate_ids'] = import_df.hate_ids.apply(lambda elem: elem.strip(\"[]\").split(\", \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comprehensive-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare thread id to hate_id and label as hateful if there is a match\n",
    "import_df['label'] = import_df.apply(lambda row: 1 if str(row.id) in row.hate_ids else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sexual-promise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    19162\n",
       "1    14614\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# desired output (from paper): 14614 hateful posts\n",
    "import_df.groupby('label').label.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "broadband-huntington",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. i joined gab to remind myself how retarded ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. This is what the left is really scared of. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>That literally looks like a monkey. Why are we...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tDumb Cunt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. It makes you an asshole.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33771</th>\n",
       "      <td>\\tfucking retard</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33772</th>\n",
       "      <td>1. Twitter BANNED me and reported me to the FB...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33773</th>\n",
       "      <td>I was routinely suspended for saying fuck, and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33774</th>\n",
       "      <td>\\tThey got me for dropping the cunt bomb... a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33775</th>\n",
       "      <td>1. How absolutely unbelievably fascinating? A ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33776 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      1. i joined gab to remind myself how retarded ...      1\n",
       "1      1. This is what the left is really scared of. ...      0\n",
       "2      That literally looks like a monkey. Why are we...      0\n",
       "3                                           \\tDumb Cunt       1\n",
       "4                           1. It makes you an asshole.       0\n",
       "...                                                  ...    ...\n",
       "33771                                  \\tfucking retard       1\n",
       "33772  1. Twitter BANNED me and reported me to the FB...      0\n",
       "33773  I was routinely suspended for saying fuck, and...      1\n",
       "33774  \\tThey got me for dropping the cunt bomb... a ...      1\n",
       "33775  1. How absolutely unbelievably fascinating? A ...      1\n",
       "\n",
       "[33776 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select relevant columns and write to copy\n",
    "gq_df = import_df[['text', 'label']].copy()\n",
    "\n",
    "gq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-black",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sexual-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33776 posts, of which 982 were dropped for empty string content\n",
      "32794 posts remain. \n",
      "\n",
      "CPU times: user 1.45 s, sys: 22.6 ms, total: 1.48 s\n",
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define function to clean text\n",
    "def clean(text):\n",
    "\n",
    "    # convert html\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # replace mentions, URLs and emojis with special token\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    text = ''.join('[EMOJI]' if (char in emoji.UNICODE_EMOJI['en']) else char for char in text).strip()\n",
    "    \n",
    "    # clean misformatting (e.g. \"\\xa0\")\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    \n",
    "    # remove newline and tab characters\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    \n",
    "    # remove leading \"1. \" (data artifact):\n",
    "    if text.startswith('1.'):\n",
    "        text = text[len('1.'):]\n",
    "        \n",
    "    # collapse whitespace into single whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# create clean_text column\n",
    "gq_df['clean_text'] = gq_df.text.apply(clean)\n",
    "\n",
    "# save number of documents before dropping empty posts\n",
    "n_docs = gq_df.shape[0]\n",
    "\n",
    "# drop rows with empty text\n",
    "gq_df = gq_df[gq_df.clean_text!=\"\"]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - gq_df.shape[0]} were dropped for empty string content')\n",
    "print(f'{gq_df.shape[0]} posts remain. \\n')\n",
    "\n",
    "# save number of documents before dropping posts that are just [URL], [EMOJI] or [USER]\n",
    "n_docs = gq_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "careful-paste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[USER] contained in 17238 documents\n",
      "[URL] contained in 13596 documents\n",
      "[EMOJI] contained in 20974 documents\n"
     ]
    }
   ],
   "source": [
    "# get overview of frequency of special tokens\n",
    "for special_token in ['[USER]', '[URL]', '[EMOJI]']:\n",
    "    print(f'{special_token} contained in {gq_df[gq_df.clean_text.str.contains(special_token)].shape[0]} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-frank",
   "metadata": {},
   "source": [
    "## TOTAL: Split and save sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sharp-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full df\n",
    "export_train, export_eval = train_test_split(gq_df[['clean_text', 'label']], train_size = 0.8, stratify = gq_df.label, random_state = 123)\n",
    "export_train.to_csv('../../0_data/clean/labelled_gq/train_rand_26k.csv', index=False)\n",
    "export_eval.to_csv('../../0_data/clean/labelled_gq/test_rand_6k.csv', index=False)\n",
    "    \n",
    "for size, name in [(20000, \"20k\"), (10000, \"10k\"), (5000, \"5k\"), (2000, \"2k\"), (1000, \"1k\")]:\n",
    "    export_sample_train, _ = train_test_split(export_train, train_size = size, stratify = export_train.label, random_state = 123)\n",
    "    export_train.to_csv(f'../../0_data/clean/labelled_gq/train_rand_{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-yukon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
