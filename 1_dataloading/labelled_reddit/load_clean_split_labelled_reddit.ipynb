{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "apart-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import csv\n",
    "import re\n",
    "import fasttext\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "from psaw import PushshiftAPI\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "british-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "useful-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE_DONALD\n",
      "2017 8\n",
      "2018 2\n",
      "2018 8\n",
      "2019 2\n",
      "2019 8\n",
      "2020 2\n",
      "\n",
      "LIBERTARIAN\n",
      "2017 8\n",
      "2018 2\n",
      "2018 8\n",
      "2019 2\n",
      "2019 8\n",
      "2020 2\n",
      "\n",
      "CONSERVATIVE\n",
      "2017 8\n",
      "2018 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Paul/opt/miniconda3/envs/language_change/lib/python3.8/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/Users/Paul/opt/miniconda3/envs/language_change/lib/python3.8/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 8\n",
      "2019 2\n",
      "2019 8\n",
      "2020 2\n",
      "\n",
      "POLITICS\n",
      "2017 8\n",
      "2018 2\n",
      "2018 8\n",
      "2019 2\n",
      "2019 8\n",
      "2020 2\n",
      "\n",
      "CHAPOTRAPHOUSE\n",
      "2017 8\n",
      "2018 2\n",
      "2018 8\n",
      "2019 2\n",
      "2019 8\n",
      "2020 2\n",
      "\n",
      "CPU times: user 1min 46s, sys: 8.34 s, total: 1min 54s\n",
      "Wall time: 51min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialise empty df for writing into\n",
    "sample_df = pd.DataFrame(columns = ['created_utc', 'body', 'subreddit'])\n",
    "\n",
    "for subreddit in ['the_donald', 'libertarian', 'conservative', 'politics', 'chapotraphouse']:\n",
    "    print(subreddit.upper())\n",
    "    for year, month in [(2017, 8), (2018, 2), (2018, 8), (2019, 2), (2019, 8), (2020, 2)]:\n",
    "        print(year, month)\n",
    "        for day in range(1, 29):\n",
    "            sample_time_epoch = int(dt.datetime(year = year, month = month, day = day, hour = 12, minute = 0 ).timestamp())\n",
    "\n",
    "            sample_df = sample_df.append(pd.DataFrame(api.search_comments(before = sample_time_epoch,\n",
    "                                                                          subreddit = subreddit,\n",
    "                                                                          filter = ['body', 'subreddit'],\n",
    "                                                                          limit = 300))[['created_utc', 'body', 'subreddit']])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "short-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "female-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert epoch to readable datetime\n",
    "reddit_df.created_utc = reddit_df.created_utc.apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# create monthyear column for easier sorting\n",
    "reddit_df['monthyear'] = reddit_df.created_utc.apply(lambda x: x.to_period('M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respiratory-puppy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit\n",
      "ChapoTrapHouse    50400\n",
      "Conservative      50400\n",
      "Libertarian       50400\n",
      "The_Donald        50400\n",
      "politics          50400\n",
      "Name: subreddit, dtype: int64\n",
      "\n",
      "monthyear\n",
      "2017-08    42000\n",
      "2018-02    42000\n",
      "2018-08    42000\n",
      "2019-02    42000\n",
      "2019-08    42000\n",
      "2020-02    42000\n",
      "Freq: M, Name: monthyear, dtype: int64\n",
      "\n",
      "monthyear  subreddit     \n",
      "2017-08    ChapoTrapHouse    8400\n",
      "           Conservative      8400\n",
      "           Libertarian       8400\n",
      "           The_Donald        8400\n",
      "           politics          8400\n",
      "2018-02    ChapoTrapHouse    8400\n",
      "           Conservative      8400\n",
      "           Libertarian       8400\n",
      "           The_Donald        8400\n",
      "           politics          8400\n",
      "2018-08    ChapoTrapHouse    8400\n",
      "           Conservative      8400\n",
      "           Libertarian       8400\n",
      "           The_Donald        8400\n",
      "           politics          8400\n",
      "2019-02    ChapoTrapHouse    8400\n",
      "           Conservative      8400\n",
      "           Libertarian       8400\n",
      "           The_Donald        8400\n",
      "           politics          8400\n",
      "2019-08    ChapoTrapHouse    8400\n",
      "           Conservative      8400\n",
      "           Libertarian       8400\n",
      "           The_Donald        8400\n",
      "           politics          8400\n",
      "2020-02    ChapoTrapHouse    8400\n",
      "           Conservative      8400\n",
      "           Libertarian       8400\n",
      "           The_Donald        8400\n",
      "           politics          8400\n",
      "Name: monthyear, dtype: int64\n",
      "\n",
      "created_utc\n",
      "2017-08-01    1500\n",
      "2017-08-02    1500\n",
      "2017-08-03    1500\n",
      "2017-08-04    1500\n",
      "2017-08-05    1500\n",
      "              ... \n",
      "2020-02-24    1500\n",
      "2020-02-25    1500\n",
      "2020-02-26    1500\n",
      "2020-02-27    1500\n",
      "2020-02-28    1500\n",
      "Freq: D, Name: created_utc, Length: 168, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# sanity checks:\n",
    "def sanity_check(df):    \n",
    "    print(df.groupby('subreddit').subreddit.count())\n",
    "    print()\n",
    "    print(df.groupby('monthyear').monthyear.count())\n",
    "    print()\n",
    "    print(df.groupby(['monthyear', 'subreddit']).monthyear.count())\n",
    "    print()\n",
    "    print(df.groupby(df.created_utc.apply(lambda x: x.to_period('D'))).created_utc.count())\n",
    "    \n",
    "sanity_check(reddit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-rental",
   "metadata": {},
   "source": [
    "## Clean text\n",
    "with cutoff at max_length to avoid super long / spam comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "round-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 60.7 ms, total: 10.5 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define function to clean text\n",
    "def clean(text, max_length = 1024):\n",
    "\n",
    "    # convert html\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # clean unicode formatting errors\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.replace('\\u200d', '')\n",
    "    \n",
    "    # truncate text to max_length\n",
    "    text = text[:max_length]\n",
    "    \n",
    "    # remove newline and tab characters\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    \n",
    "    # replace URLs and emojis with special tokens\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    text = ''.join('[EMOJI]' if (char in emoji.UNICODE_EMOJI['en']) else char for char in text).strip()\n",
    "    \n",
    "    # remove deleted posts\n",
    "    text = text.replace('[removed]','')\n",
    "        \n",
    "    # collapse whitespace into single whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # truncate text to max_length again\n",
    "    text = text[:max_length]\n",
    "    \n",
    "    # remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# create clean_text column\n",
    "reddit_df['clean_text'] = reddit_df.body.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "painted-championship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252000 posts, of which 12362 were dropped for empty string content\n",
      "239638 posts remain. \n",
      "\n",
      "CPU times: user 87.6 ms, sys: 10.1 ms, total: 97.7 ms\n",
      "Wall time: 98.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# save number of documents before dropping empty posts\n",
    "n_docs = reddit_df.shape[0]\n",
    "\n",
    "# drop rows with empty text\n",
    "reddit_df = reddit_df[reddit_df.clean_text.values!=\"\"]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - reddit_df.shape[0]} were dropped for empty string content')\n",
    "print(f'{reddit_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surprised-purpose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239638 posts, of which 989 were dropped for being just [URL] or [EMOJI]\n",
      "238649 posts remain. \n",
      "\n",
      "CPU times: user 92.5 ms, sys: 4.22 ms, total: 96.7 ms\n",
      "Wall time: 95.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# save number of documents before dropping posts that are just [URL] or [EMOJI]\n",
    "n_docs = reddit_df.shape[0]\n",
    "\n",
    "# drop rows with text that is just [URL] or [EMOJI]\n",
    "reddit_df = reddit_df[(reddit_df.clean_text!=\"[URL]\") & (reddit_df.clean_text!=\"[EMOJI]\")]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - reddit_df.shape[0]} were dropped for being just [URL] or [EMOJI]')\n",
    "print(f'{reddit_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inside-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238649 posts, of which 2719 were dropped for (most likely) not being in English.\n",
      "235930 posts remain. \n",
      "\n",
      "CPU times: user 9.69 s, sys: 116 ms, total: 9.8 s\n",
      "Wall time: 9.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# check language\n",
    "fmodel = fasttext.load_model('../../0_models/lang_detect/lid.176.bin')\n",
    "\n",
    "def check_language(text):\n",
    "    predictions = fmodel.predict(text, k=3)\n",
    "    \n",
    "    # if top prediction is certain and not English, return non-English\n",
    "    if (predictions[0][0]!='__label__en') and (predictions[1][0]>0.50):\n",
    "        return 'non-English'\n",
    "    \n",
    "    # else if English is one of top 3 predictions, return English\n",
    "    elif '__label__en' in predictions[0]:\n",
    "        return 'English'\n",
    "    \n",
    "    # else return non-English\n",
    "    else:\n",
    "        return 'non-English'\n",
    "\n",
    "# save number of documents before dropping non-English posts\n",
    "n_docs = reddit_df.shape[0]\n",
    "\n",
    "# drop non-English posts\n",
    "reddit_df = reddit_df[reddit_df.clean_text.apply(lambda x: check_language(x) == 'English')]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - reddit_df.shape[0]} were dropped for (most likely) not being in English.')\n",
    "print(f'{reddit_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "laughing-pastor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235930 posts, of which 14221 were dropped for being duplicates.\n",
      "221709 posts remain. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save number of documents before dropping duplicates\n",
    "n_docs = reddit_df.shape[0]\n",
    "\n",
    "# drop duplicates\n",
    "reddit_df.drop_duplicates(subset = ['clean_text'], inplace=True)\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - reddit_df.shape[0]} were dropped for being duplicates.')\n",
    "print(f'{reddit_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-provision",
   "metadata": {},
   "source": [
    "## Export to CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "honey-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test splits for each month-year\n",
    "for monthyear in pd.unique(reddit_df['monthyear']):\n",
    "    \n",
    "    export_train = pd.DataFrame(columns=['clean_text', 'subreddit'])\n",
    "    export_test = export_train.copy()\n",
    "    \n",
    "    for subreddit in pd.unique(reddit_df['subreddit']):\n",
    "        \n",
    "        add_train, add_test = train_test_split(reddit_df[(reddit_df['monthyear']==monthyear)&(reddit_df['subreddit']==subreddit)][['clean_text', 'subreddit']],\n",
    "                                                     train_size = 4000, test_size = 1000,\n",
    "                                                     random_state = 123)\n",
    "        \n",
    "        export_train = export_train.append(add_train)\n",
    "        export_test = export_test.append(add_test)\n",
    "        \n",
    "    export_train.sample(frac=1).rename(columns={'subreddit': 'label'}).to_csv(f'../../0_data/clean/labelled_reddit/train-{str(monthyear)[-5:]}.csv', index=False)\n",
    "    export_test.sample(frac=1).rename(columns={'subreddit': 'label'}).to_csv(f'../../0_data/clean/labelled_reddit/test-{str(monthyear)[-5:]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "progressive-office",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monthyear  subreddit     \n",
      "2017-08    ChapoTrapHouse    7559\n",
      "           Conservative      7097\n",
      "           Libertarian       7764\n",
      "           The_Donald        7256\n",
      "           politics          7308\n",
      "2018-02    ChapoTrapHouse    7497\n",
      "           Conservative      6867\n",
      "           Libertarian       7797\n",
      "           The_Donald        7434\n",
      "           politics          7423\n",
      "2018-08    ChapoTrapHouse    7015\n",
      "           Conservative      6406\n",
      "           Libertarian       7647\n",
      "           The_Donald        7187\n",
      "           politics          7062\n",
      "2019-02    ChapoTrapHouse    7584\n",
      "           Conservative      6664\n",
      "           Libertarian       8152\n",
      "           The_Donald        7995\n",
      "           politics          7579\n",
      "2019-08    ChapoTrapHouse    7287\n",
      "           Conservative      6773\n",
      "           Libertarian       8035\n",
      "           The_Donald        7046\n",
      "           politics          7496\n",
      "2020-02    ChapoTrapHouse    7912\n",
      "           Conservative      6994\n",
      "           Libertarian       8137\n",
      "           The_Donald        7166\n",
      "           politics          7570\n",
      "Name: monthyear, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(reddit_df.groupby(['monthyear', 'subreddit']).monthyear.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-bedroom",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
