{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collective-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import fasttext\n",
    "import emoji\n",
    "import unicodedata\n",
    "\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-federation",
   "metadata": {},
   "source": [
    "## Load Unlabelled Gab Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hungry-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "CPU times: user 3min 1s, sys: 6.85 s, total: 3min 8s\n",
      "Wall time: 3min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load 2018 texts from unlabelled corpus into set\n",
    "\n",
    "# initialise empty lists --> faster than appending to dict\n",
    "texts = []\n",
    "dates = []\n",
    "\n",
    "# initialise counter var for counting iterations\n",
    "counter = 0\n",
    "sample_freq = 1\n",
    "\n",
    "print_freq = 1000000 # print progress every n posts with n = print_freq\n",
    "\n",
    "# iterate over each line\n",
    "with open('../../0_data/raw/gabposts_clean_170221.csv', 'r') as read_obj:\n",
    "    csv_dict_reader = csv.DictReader(x.replace('\\0', '') for x in read_obj)\n",
    "    for row in csv_dict_reader:\n",
    "        if (row['created_at'] >= '2018') and (counter % sample_freq) == 0:\n",
    "            texts.append(row['text'])\n",
    "            dates.append(row['created_at'])\n",
    "        counter+=1\n",
    "        if counter % print_freq == 0:\n",
    "            print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ideal-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.7 s, sys: 2min 52s, total: 3min 26s\n",
      "Wall time: 4min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching American wierwolf in London hahah bet...</td>\n",
       "      <td>2018-01-01 00:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Happy New Year everybody https://hooktube.com/...</td>\n",
       "      <td>2018-01-01 00:00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Praying! Thank God she's alive.</td>\n",
       "      <td>2018-01-01 00:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2018-01-01 00:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump Vindicated Again! https://www.youtube.co...</td>\n",
       "      <td>2018-01-01 00:00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20360810</th>\n",
       "      <td>Just one,'' I'm better than you'' idiots opini...</td>\n",
       "      <td>2018-10-29 03:01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20360811</th>\n",
       "      <td>Very true.. #BigPharma #corruption</td>\n",
       "      <td>2018-10-29 03:02:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20360812</th>\n",
       "      <td>\"Finkelstein recently studied millions of comm...</td>\n",
       "      <td>2018-10-29 03:03:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20360813</th>\n",
       "      <td>https://thedailycoin.org/2018/10/28/movie-prop...</td>\n",
       "      <td>2018-10-29 03:05:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20360814</th>\n",
       "      <td>@a, Psalm #2... confidence is high, don't let ...</td>\n",
       "      <td>2018-10-29 03:14:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20360815 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text  \\\n",
       "0         watching American wierwolf in London hahah bet...   \n",
       "1         Happy New Year everybody https://hooktube.com/...   \n",
       "2                           Praying! Thank God she's alive.   \n",
       "3                                                             \n",
       "4         Trump Vindicated Again! https://www.youtube.co...   \n",
       "...                                                     ...   \n",
       "20360810  Just one,'' I'm better than you'' idiots opini...   \n",
       "20360811                 Very true.. #BigPharma #corruption   \n",
       "20360812  \"Finkelstein recently studied millions of comm...   \n",
       "20360813  https://thedailycoin.org/2018/10/28/movie-prop...   \n",
       "20360814  @a, Psalm #2... confidence is high, don't let ...   \n",
       "\n",
       "                  created_at  \n",
       "0        2018-01-01 00:00:01  \n",
       "1        2018-01-01 00:00:02  \n",
       "2        2018-01-01 00:00:04  \n",
       "3        2018-01-01 00:00:04  \n",
       "4        2018-01-01 00:00:04  \n",
       "...                      ...  \n",
       "20360810 2018-10-29 03:01:47  \n",
       "20360811 2018-10-29 03:02:17  \n",
       "20360812 2018-10-29 03:03:58  \n",
       "20360813 2018-10-29 03:05:45  \n",
       "20360814 2018-10-29 03:14:54  \n",
       "\n",
       "[20360815 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dataframe from lists\n",
    "texts = pd.Series(texts, name = 'text')\n",
    "dates = pd.Series(dates, name = 'created_at')\n",
    "sample_df = pd.concat([texts, dates], axis=1)\n",
    "\n",
    "# clear out RAM\n",
    "del texts\n",
    "del dates\n",
    "\n",
    "# convert dtypes\n",
    "sample_df['created_at']= sample_df.created_at.astype('datetime64')\n",
    "sample_df['text']= sample_df.text.astype('string')\n",
    "\n",
    "# print finished df\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-clock",
   "metadata": {},
   "source": [
    "## Perform Additional Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "executed-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy of sample_df to avoid having to reload sample_df\n",
    "text_df = sample_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "painful-laugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 18s, sys: 1min 27s, total: 8min 46s\n",
      "Wall time: 11min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define function to clean text\n",
    "def clean(text):\n",
    "\n",
    "    # convert html\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # replace mentions, URLs and emojis with special token\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    text = ''.join('[EMOJI]' if (char in emoji.UNICODE_EMOJI['en']) else char for char in text).strip()\n",
    "    \n",
    "    # clean misformatting (e.g. \"\\xa0\")\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# create clean_text column\n",
    "text_df['clean_text'] = text_df.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "funded-victory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20360815 posts, of which 1187729 were dropped for empty string content\n",
      "19173086 posts remain. \n",
      "\n",
      "CPU times: user 10.7 s, sys: 1min 32s, total: 1min 43s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# save number of documents before dropping empty posts\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop rows with empty text\n",
    "text_df = text_df[text_df.clean_text.values!=\"\"]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for empty string content')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effective-welcome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19173086 posts, of which 1598858 were dropped for being just [URL], [EMOJI] or [USER]\n",
      "17574228 posts remain. \n",
      "\n",
      "CPU times: user 16.1 s, sys: 1min 48s, total: 2min 5s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# save number of documents before dropping posts that are just [URL], [EMOJI] or [USER]\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop rows with text that is just [URL], [EMOJI] or [USER]\n",
    "text_df = text_df[(text_df.clean_text!=\"[URL]\") & (text_df.clean_text!=\"[EMOJI]\") & (text_df.clean_text!=\"[USER]\")]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for being just [URL], [EMOJI] or [USER]')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finished-thomson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17574228 posts, of which 1423284 were dropped for (most likely) not being in English.\n",
      "16150944 posts remain. \n",
      "\n",
      "CPU times: user 13min 34s, sys: 2min 28s, total: 16min 2s\n",
      "Wall time: 20min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# check language\n",
    "fmodel = fasttext.load_model('../../0_models/lang_detect/lid.176.bin')\n",
    "\n",
    "def check_language(text):\n",
    "    predictions = fmodel.predict(text, k=3)\n",
    "    \n",
    "    # if top prediction is certain and not English, return non-English\n",
    "    if (predictions[0][0]!='__label__en') and (predictions[1][0]>0.50):\n",
    "        return 'non-English'\n",
    "    \n",
    "    # else if English is one of top 3 predictions, return English\n",
    "    elif '__label__en' in predictions[0]:\n",
    "        return 'English'\n",
    "    \n",
    "    # else return non-English\n",
    "    else:\n",
    "        return 'non-English'\n",
    "\n",
    "# save number of documents before dropping non-English posts\n",
    "n_docs = text_df.shape[0]\n",
    "\n",
    "# drop non-English posts\n",
    "text_df = text_df[text_df.text.apply(lambda x: check_language(x) == 'English')]\n",
    "\n",
    "print(f'{n_docs} posts, of which {n_docs - text_df.shape[0]} were dropped for (most likely) not being in English.')\n",
    "print(f'{text_df.shape[0]} posts remain. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-width",
   "metadata": {},
   "source": [
    "## Write to Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thousand-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 14s, sys: 1min 44s, total: 6min 58s\n",
      "Wall time: 8min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create monthyear column for easier sorting\n",
    "text_df['monthyear'] = text_df.created_at.apply(lambda x: x.to_period('M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fifty-favor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.4 s, sys: 52.8 s, total: 1min 17s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## monthly train and test sets\n",
    "\n",
    "TRAIN_SIZE = 1000000\n",
    "TEST_SIZE = 10000\n",
    "\n",
    "export_train = {}\n",
    "export_test = {}\n",
    "\n",
    "for my in pd.unique(text_df.monthyear):\n",
    "    export_train[my], export_test[my] = train_test_split(text_df[text_df.monthyear==my].sample(TRAIN_SIZE+TEST_SIZE, random_state=123).clean_text,\n",
    "                                                 train_size = TRAIN_SIZE, test_size = TEST_SIZE, random_state=123)\n",
    "    \n",
    "    with open(f'../../0_data/clean/unlabelled_pushshift/month_splits/train_{my}_{int(TRAIN_SIZE/1000000)}m.txt', 'w') as write_obj:\n",
    "        for text in export_train[my]:\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "            \n",
    "    with open(f'../../0_data/clean/unlabelled_pushshift/month_splits/test_{my}_{int(TEST_SIZE/1000)}k.txt', 'w') as write_obj:\n",
    "        for text in export_test[my]:\n",
    "            write_obj.write(text + \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "educational-apollo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 s, sys: 21.7 s, total: 51.7 s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# random total train and test sets stratified by time\n",
    "\n",
    "TRAIN_SIZES = [1000000, 2000000, 5000000, 10000000]\n",
    "TEST_SIZE = 10000\n",
    "\n",
    "# train sets\n",
    "for size in TRAIN_SIZES: \n",
    "\n",
    "    export_train['rand'] = pd.Series(dtype=str)\n",
    "    \n",
    "    for my in pd.unique(text_df.monthyear):\n",
    "        export_train['rand'] = export_train['rand'].append(export_train[my].sample(int(size/10), random_state=123), ignore_index=True)\n",
    "        \n",
    "    with open(f'../../0_data/clean/unlabelled_pushshift/month_splits/total/train_rand_{int(size/1000000)}m.txt', 'w') as write_obj:\n",
    "        for text in export_train['rand']:\n",
    "            write_obj.write(text + \"\\n \\n\")\n",
    "            \n",
    "            \n",
    "# test set\n",
    "\n",
    "export_test['rand'] = pd.Series(dtype=str)\n",
    "\n",
    "for my in pd.unique(text_df.monthyear):\n",
    "    export_test['rand'] = export_test['rand'].append(export_test[my].sample(int(TEST_SIZE/10), random_state=123), ignore_index=True)\n",
    "\n",
    "with open(f'../../0_data/clean/unlabelled_pushshift/month_splits/total/test_rand_{int(TEST_SIZE/1000)}k.txt', 'w') as write_obj:\n",
    "    for text in export_test['rand']:\n",
    "        write_obj.write(text + \"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-broadway",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
